{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hXgEgZ1Gzvm",
        "outputId": "39b733c1-9b1d-4e3c-8b68-edf2a91b238b"
      },
      "id": "9hXgEgZ1Gzvm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract keywords"
      ],
      "metadata": {
        "id": "bRAVwn1PHgQE"
      },
      "id": "bRAVwn1PHgQE"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 1: Install spaCy and download English model\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import spacy\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import files\n",
        "\n",
        "# Step 3: Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 4: Read the file named teext.txt\n",
        "with open(\"ISOIEC19086-22018.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Step 5: Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Step 6: Extract keywords (nouns and proper nouns, excluding stop words and punctuation)\n",
        "keywords = {token.text.lower() for token in doc if token.pos_ in (\"NOUN\", \"PROPN\") and not token.is_stop and token.is_alpha}\n",
        "\n",
        "# Step 7: Build XML structure\n",
        "root = ET.Element(\"keywords\")\n",
        "for kw in sorted(keywords):\n",
        "    ET.SubElement(root, \"keyword\").text = kw\n",
        "\n",
        "# Step 8: Write to XML file\n",
        "tree = ET.ElementTree(root)\n",
        "output_filename = \"keywords.xml\"\n",
        "tree.write(output_filename, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "# Step 9: Download the XML file\n",
        "files.download(output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PWMRFKv4G1RP",
        "outputId": "267aac72-756e-4d97-84ad-a035ebb333f3"
      },
      "id": "PWMRFKv4G1RP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3b222f97-aa32-4cb0-ab15-cf05be9c7147\", \"keywords.xml\", 5436)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT:**\n",
        "\n",
        "```\n",
        "<keywords>\n",
        "<keyword>accordance</keyword>\n",
        "<keyword>addresses</keyword>\n",
        "<keyword>adherence</keyword>\n",
        "<keyword>agreement</keyword>\n",
        "<keyword>agreements</keyword>\n",
        "<keyword>ambiguities</keyword>\n",
        "<keyword>amendments</keyword>\n",
        "<keyword>applications</keyword>\n",
        "<keyword>approach</keyword>\n",
        "<keyword>approval</keyword>\n",
        "<keyword>assessment</keyword>\n",
        "<keyword>attention</keyword>\n",
        "<keyword>availability</keyword>\n",
        "<keyword>barriers</keyword>\n",
        "<keyword>benefit</keyword>\n",
        "<keyword>bodies</keyword>\n",
        "<keyword>body</keyword>\n",
        "<keyword>business</keyword>\n",
        "<keyword>cases</keyword>\n",
        "<keyword>challenges</keyword>\n",
        "<keyword>characteristic</keyword>\n",
        "<keyword>clarity</keyword>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BPuA_uiXI_1D"
      },
      "id": "BPuA_uiXI_1D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract entities: Named Entity Recognition (NER) feature.\n",
        "\n",
        "üîç spaCy NER Entities Examples:\n",
        "PERSON ‚Äì People names\n",
        "\n",
        "ORG ‚Äì Organizations\n",
        "\n",
        "GPE ‚Äì Countries, cities, states\n",
        "\n",
        "DATE, TIME, MONEY, etc."
      ],
      "metadata": {
        "id": "aofgPLNaHkFd"
      },
      "id": "aofgPLNaHkFd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 4: Read the file named teext.txt\n",
        "with open(\"ISOIEC19086-22018.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Step 5: Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Step 6: Extract named entities (grouped by label)\n",
        "entities_by_label = {}\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in entities_by_label:\n",
        "        entities_by_label[label] = set()\n",
        "    entities_by_label[label].add(ent.text.strip())\n",
        "\n",
        "# Step 7: Build XML structure\n",
        "root = ET.Element(\"named_entities\")\n",
        "\n",
        "for label, entities in sorted(entities_by_label.items()):\n",
        "    label_elem = ET.SubElement(root, label)\n",
        "    for entity in sorted(entities):\n",
        "        ET.SubElement(label_elem, \"entity\").text = entity\n",
        "\n",
        "# Step 8: Write to XML file\n",
        "tree = ET.ElementTree(root)\n",
        "output_filename = \"entities.xml\"\n",
        "tree.write(output_filename, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "# Step 9: Download the XML file\n",
        "files.download(output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mejHzu0aHOZY",
        "outputId": "0574a2a4-be40-4192-93ed-e3b1ffced630"
      },
      "id": "mejHzu0aHOZY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_33ee19a5-5a2d-4991-a1db-c3098da7575e\", \"entities.xml\", 1577)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output:**\n",
        "\n",
        "```\n",
        "<named_entities>\n",
        "<CARDINAL>\n",
        "<entity>1</entity>\n",
        "<entity>1 to 5</entity>\n",
        "<entity>2</entity>\n",
        "<entity>3</entity>\n",
        "<entity>3.1</entity>\n",
        "<entity>3.10</entity>\n",
        "<entity>3.2</entity>\n",
        "<entity>3.3</entity>\n",
        "<entity>3.4</entity>\n",
        "<entity>3.5</entity>\n",
        "<entity>3.6</entity>\n",
        "<entity>3.7</entity>\n",
        "<entity>one</entity>\n",
        "</CARDINAL>\n",
        "<DATE>\n",
        "<entity>19086</entity>\n",
        "<entity>19086-1</entity>\n",
        "<entity>19086-1:2016</entity>\n",
        "<entity>19086-3</entity>\n",
        "<entity>19086-4</entity>\n",
        "<entity>October 2004</entity>\n",
        "</DATE>\n",
        "<LANGUAGE>\n",
        "<entity>English</entity>\n",
        "</LANGUAGE>\n",
        "<ORDINAL>\n",
        "<entity>first</entity>\n",
        "<entity>second</entity>\n",
        "</ORDINAL>\n",
        "<ORG>\n",
        "<entity>CSP</entity>\n",
        "<entity>Foreword ISO</entity>\n",
        "<entity>IEC</entity>\n",
        "<entity>IEC Electropedia</entity>\n",
        "<entity>ISO</entity>\n",
        "<entity>ISO/IEC</entity>\n",
        "<entity>ISO/IEC 17788</entity>\n",
        "<entity>ISO/IEC 80000-1:2009</entity>\n",
        "<entity>ISO/IEC JTC 1</entity>\n",
        "<entity>ISO/IEC/IEEE</entity>\n",
        "<entity>ITU</entity>\n",
        "<entity>International Standards</entity>\n",
        "<entity>Subcommittee SC 38</entity>\n",
        "<entity>Technical Committee ISO/IEC JTC1, Information technology</entity>\n",
        "<entity>WTO</entity>\n",
        "<entity>http://www.iso.org/obp</entity>\n",
        "<entity>the International Organization for Standardization</entity>\n",
        "<entity>the Technical Barriers to Trade (TBT</entity>\n",
        "<entity>the World Trade Organization</entity>\n",
        "</ORG>\n",
        "<PERCENT>\n",
        "<entity>100 %</entity>\n",
        "</PERCENT>\n",
        "<PERSON>\n",
        "<entity>Cloud Computing</entity>\n",
        "<entity>XML Schema Part</entity>\n",
        "</PERSON>\n",
        "<PRODUCT>\n",
        "<entity>SOs</entity>\n",
        "</PRODUCT>\n",
        "<QUANTITY>\n",
        "<entity>3.6 metric</entity>\n",
        "</QUANTITY>\n",
        "<WORK_OF_ART>\n",
        "<entity>Datatypes Second Edition</entity>\n",
        "</WORK_OF_ART>\n",
        "</named_entities>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gvxztIFzJMQI"
      },
      "id": "gvxztIFzJMQI"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Load the text\n",
        "with open(\"ISOIEC19086-22018.txt\", \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Create root element for XML\n",
        "root = ET.Element(\"requirements\")\n",
        "\n",
        "# Define basic patterns to match requirements\n",
        "patterns = [\n",
        "    (r\"latency.*?(less than|under|<)\\s*(\\d+\\.?\\d*)\\s*ms\", \"latency\"),\n",
        "    (r\"availability.*?(\\d+\\.?\\d*)\\s*%\", \"availability\"),\n",
        "    (r\"jitter.*?(up to|less than|<)\\s*(\\d+\\.?\\d*)\\s*ms\", \"jitter\")\n",
        "]\n",
        "\n",
        "# Process each line\n",
        "for line in lines:\n",
        "    print(f\"Processing line: {line.strip()}\")\n",
        "    for pattern, req_type in patterns:\n",
        "        match = re.search(pattern, line, re.IGNORECASE)\n",
        "        if match:\n",
        "            print(f\" Matched {req_type}: {match.groupdict()}\")\n",
        "            req_elem = ET.SubElement(root, \"requirement\")\n",
        "            ET.SubElement(req_elem, \"type\").text = req_type\n",
        "\n",
        "            # Safely extract named groups\n",
        "            value = match.groupdict().get(\"value\")\n",
        "            operator = match.groupdict().get(\"operator\")\n",
        "\n",
        "            if value:\n",
        "                ET.SubElement(req_elem, \"constraint\").text = value\n",
        "            else:\n",
        "                print(\" No 'value' group found for match:\", match.group())\n",
        "\n",
        "            if operator:\n",
        "                req_elem.set(\"operator\", operator)\n",
        "\n",
        "# Save to XML\n",
        "tree = ET.ElementTree(root)\n",
        "tree.write(\"output.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "print(\" XML generated as 'output.xml'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7QnlD8rH4VG",
        "outputId": "f9558540-e79c-46aa-f2fd-eaa970a6222b"
      },
      "id": "b7QnlD8rH4VG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing line: Foreword\n",
            "Processing line: ISO (the International Organization for Standardization) is a worldwide federation of national standards bodies (ISO member bodies). The work of preparing International Standards is normally carried out through ISO technical committees. Each member body interested in a subject for which a technical committee has been established has the right to be represented on that committee. International organizations, governmental and non-governmental, in liaison with ISO, also take part in the work. In the field of information technology, ISO and IEC have established a joint technical committee, ISO/IEC JTC 1\n",
            "Processing line: The procedures used to develop this document and those intended for its further maintenance are described in the ISO/IEC Directives, Part 1. In particular, the different approval criteria needed for the different types of ISO documents should be noted. This document was drafted in accordance with the editorial rules of the ISO/IEC Directives, Part 2 (see www.iso.org/directives).\n",
            "Processing line: Attention is drawn to the possibility that some of the elements of this document may be the subject of patent rights. ISO shall not be held responsible for identifying any or all such patent rights. Details of any patent rights identified during the development of the document will be in the Introduction and/or on the ISO list of patent declarations received (see www.iso.org/patents).\n",
            "Processing line: Any trade name used in this document is information given for the convenience of users and does not constitute an endorsement.\n",
            "Processing line: For an explanation of the voluntary nature of standards, the meaning of ISO specific terms and expressions related to conformity assessment, as well as information about ISO's adherence to the World Trade Organization (WTO) principles in the Technical Barriers to Trade (TBT) see www.iso.org/iso/foreword.html.\n",
            "Processing line: This document was prepared by Technical Committee ISO/IEC JTC1, Information technology, Subcommittee SC 38, Cloud Computing and Distributed Platforms.\n",
            "Processing line: A list of all parts in the ISO 19086 series can be found on the ISO website.\n",
            "Processing line: Any feedback or questions on this document should be directed to the user‚Äôs national standards body. A complete listing of these bodies can be found at www.iso.org/members.html.\n",
            "Processing line: Introduction\n",
            "Processing line: The measurement of properties of cloud services, especially for the purpose of cloud service level agreements (SLAs), presents many challenges, which inhibit the uptake of cloud services and inhibit the overall effectiveness of the cloud services marketplace. Metrics in practice are usually described using natural languages, typically in ‚Äòplain English‚Äô, which is often difficult to understand, compare, and implement. Such definitions of metrics lead to many problems. Typical concerns include:\n",
            "Processing line: ‚Äî Clarity: The metric definition may be incomplete, ambiguous, illogical, self-contradictory, or not defined at all. For example, cases exist where ‚Äòavailability‚Äô is defined in ways which have little to do with generally accepted definitions of ‚Äòavailability‚Äô; where the definition is such that the service can be unavailable for the majority of the time yet the metric will show 100 % availability; where the metric requires continuous monitoring, which is actually not possible; or where the provider is able to determine at its sole discretion what the result is.\n",
            " Matched availability: {}\n",
            " No 'value' group found for match: availability‚Äô is defined in ways which have little to do with generally accepted definitions of ‚Äòavailability‚Äô; where the definition is such that the service can be unavailable for the majority of the time yet the metric will show 100 %\n",
            "Processing line: ‚Äî Comparability: It may be impractical or effectively impossible to compare different services in terms of their promised service levels because of the significant inconsistency in how their respective metrics and SLOs/SQOs are defined.\n",
            "Processing line: ‚Äî Implementation: It may be impractical or even impossible to measure the metric in practice, and to determine whether promised service levels have been met or not.\n",
            "Processing line: This document has been developed to help address these and similar concerns. It includes technical content, but the high-level concepts are expected to be understandable by non-technical individuals who understand the business context for metrics. It provides a metric model that defines the conditions and rules for performing a measurement and understanding the result.\n",
            "Processing line: A metric complying with the model defined by this document addresses the concerns above:\n",
            "Processing line: ‚Äî Clarity: A definition of a metric eliminates the ambiguities which currently exist in natural language descriptions.\n",
            "Processing line: ‚Äî Comparability: The structured nature of the metric facilitates the comparison of different metrics and SLOs/SQOs based on a metric.\n",
            "Processing line: ‚Äî Implementation: The structured representation of the information needed to measure a characteristic facilitates the process of developing measurement tools. Likewise, if the metric is found not to be implementable, then the metric will need to be revised so that it can be implemented, and the structure of the technical specification is expected to facilitate this revision process.\n",
            "Processing line: The focus of this document is on metrics for cloud SLAs, but it is also usable for cloud service metrics (CSMs) that are not included in cloud SLAs [such as ones used by cloud service providers (CSPs) for their internal performance monitoring], and may also be usable for non-CSMs.\n",
            "Processing line: 1   Scope\n",
            "Processing line: This document establishes common terminology, defines a model for specifying metrics for cloud SLAs, and includes applications of the model with examples. This document establishes a common terminology and approach for specifying metrics.\n",
            "Processing line: This document is for the benefit of and use for both cloud service providers (CSPs) and cloud service customers (CSCs). This document is intended to complement ISO/IEC 19086-1, ISO/IEC 19086-3 and ISO/IEC 19086-4.\n",
            "Processing line: This document does not mandate the use of a specific set of metrics for cloud SLAs.\n",
            "Processing line: 2   Normative references\n",
            "Processing line: The following documents are referred to in the text in such a way that some or all of their content constitutes requirements of this document. For dated references, only the edition cited applies. For undated references, the latest edition of the referenced document (including any amendments) applies.\n",
            "Processing line: ISO/IEC 17788, | ITU-T Y.3500, Information technology ‚Äî Cloud computing ‚Äî Overview and vocabulary\n",
            "Processing line: ISO/IEC 19086-1, Information technology ‚Äî Cloud computing ‚Äî Service level agreement (SLA) framework ‚Äî Part 1: Overview and concepts\n",
            "Processing line: W3C Recommendation 28 October 2004. XML Schema Part 1: Structures Second Edition. http://www.w3.org/TR/xmlschema-1/\n",
            "Processing line: W3C Recommendation 28 October 2004. XML Schema Part 2: Datatypes Second Edition. http://www.w3.org/TR/xmlschema-2/\n",
            "Processing line: 3   Terms and definitions\n",
            "Processing line: For the purposes of this document, the terms and definitions given in ISO/IEC 17788 and the following definitions apply.\n",
            "Processing line: ISO and IEC maintain terminological databases for use in standardization at the following addresses:\n",
            "Processing line: ‚Äî IEC Electropedia: available at http://www.electropedia.org/\n",
            "Processing line: ‚Äî ISO Online browsing platform: available at http://www.iso.org/obp\n",
            "Processing line: 3.1\n",
            "Processing line: cloud service characteristic\n",
            "Processing line: qualitative or quantitative property of a cloud service\n",
            "Processing line: 3.2\n",
            "Processing line: cloud service metric\n",
            "Processing line: metric (3.6) used to assess a cloud service characteristic (3.1)\n",
            "Processing line: 3.3\n",
            "Processing line: cloud service objective\n",
            "Processing line: SO\n",
            "Processing line: commitment a cloud service provider (CSP) makes for a specific characteristic of a cloud service\n",
            "Processing line: Note 1 to entry: The set of SOs is the union of the set of cloud service level objectives (SLOs) and the set of cloud service qualitative objectives (SQOs).\n",
            "Processing line: 3.4\n",
            "Processing line: measurement\n",
            "Processing line: set of operations having the objective of determining a measurement result (3.5)\n",
            "Processing line: Note 1 to entry: Based on the definition of measurement in ISO/IEC/IEEE 15939:2017. Also used here to describe an actual instance of execution of these operations leading to the production of a measurement result instance.\n",
            "Processing line: 3.5\n",
            "Processing line: measurement result\n",
            "Processing line: value that expresses a qualitative or quantitative assessment of a cloud service characteristic (3.1)\n",
            "Processing line: 3.6\n",
            "Processing line: metric\n",
            "Processing line: standard of measurement that defines the conditions and the rules for performing the measurement (3.4) and for understanding the measurement result (3.5)\n",
            "Processing line: Note 1 to entry: The metric describes what the result of the measurement means, but not how the measurement implements the metric.\n",
            "Processing line: Note 2 to entry: A metric is to be applied in practice within a given context that requires specific properties to be measured, at a given time(s) for a specific objective.\n",
            "Processing line: Note 3 to entry: The metrics model proposed in this document supports the definition of composite metrics, which can be defined in terms of one or more underlying (reusable) metrics.\n",
            "Processing line: [SOURCE:ISO/IEC 19086-1:2016, 3.10, modified ‚Äî Notes to entry have been modified.]\n",
            "Processing line: 3.7\n",
            "Processing line: unit\n",
            "Processing line: real scalar quantity, defined and adopted by convention, with which any other quantity of the same kind can be compared to express the ratio of the second quantity to the first one as a number\n",
            "Processing line: [SOURCE:ISO/IEC 80000-1:2009, modified ‚Äî NOTES 1 to 5 have not been included.]\n",
            " XML generated as 'output.xml'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output**\n",
        "\n",
        "```\n",
        "<requirements>\n",
        "<requirement>\n",
        "<type>availability</type>\n",
        "</requirement>\n",
        "</requirements>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MK977x3gJjB3"
      },
      "id": "MK977x3gJjB3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER with SLAtext"
      ],
      "metadata": {
        "id": "DiIpNtqRKrMR"
      },
      "id": "DiIpNtqRKrMR"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Import required libraries\n",
        "import spacy\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import files\n",
        "# Step 3: Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 4: Read the file named teext.txt\n",
        "with open(\"SLAtext.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Step 5: Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Step 6: Extract named entities (grouped by label)\n",
        "entities_by_label = {}\n",
        "for ent in doc.ents:\n",
        "    label = ent.label_\n",
        "    if label not in entities_by_label:\n",
        "        entities_by_label[label] = set()\n",
        "    entities_by_label[label].add(ent.text.strip())\n",
        "\n",
        "# Step 7: Build XML structure\n",
        "root = ET.Element(\"named_entities\")\n",
        "\n",
        "for label, entities in sorted(entities_by_label.items()):\n",
        "    label_elem = ET.SubElement(root, label)\n",
        "    for entity in sorted(entities):\n",
        "        ET.SubElement(label_elem, \"entity\").text = entity\n",
        "\n",
        "# Step 8: Write to XML file\n",
        "tree = ET.ElementTree(root)\n",
        "output_filename = \"entitiesSLA.xml\"\n",
        "tree.write(output_filename, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "# Step 9: Download the XML file\n",
        "files.download(output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dElhMI39JXEm",
        "outputId": "1530e10b-9907-4aea-c8e7-b6e0c4b86065"
      },
      "id": "dElhMI39JXEm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9c1721f2-7639-4e98-a46b-6cfe2d03d02b\", \"entitiesSLA.xml\", 522)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 4: Read the file named teext.txt\n",
        "with open(\"SLAtext.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Step 5: Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Step 6: Extract keywords (nouns and proper nouns, excluding stop words and punctuation)\n",
        "keywords = {token.text.lower() for token in doc if token.pos_ in (\"NOUN\", \"PROPN\") and not token.is_stop and token.is_alpha}\n",
        "\n",
        "# Step 7: Build XML structure\n",
        "root = ET.Element(\"keywords\")\n",
        "for kw in sorted(keywords):\n",
        "    ET.SubElement(root, \"keyword\").text = kw\n",
        "\n",
        "# Step 8: Write to XML file\n",
        "tree = ET.ElementTree(root)\n",
        "output_filename = \"keywordsSLA.xml\"\n",
        "tree.write(output_filename, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "# Step 9: Download the XML file\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8OyTXWH9Ku-J",
        "outputId": "a8dcdf23-9814-4c1f-cb8e-c2341797419f"
      },
      "id": "8OyTXWH9Ku-J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64b25ce2-bf92-4100-92c6-f25b96893ac2\", \"keywordsSLA.xml\", 1014)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example**"
      ],
      "metadata": {
        "id": "rw-UlJuxOUm0"
      },
      "id": "rw-UlJuxOUm0"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load('en_core_web_sm')  # You can choose a different model as needed\n",
        "\n",
        "\n",
        "sla_text = \"\"\"\n",
        "The service provider shall ensure 99.9% uptime for the Data Backup service.\n",
        "In case of downtime, the response time should not exceed 4 hours.\n",
        "After execution of this Agreement, \"Dan\" shall pay the full purchase price to \"Jerome\" in the amount of 3.14 EUR upon demand by \"Jerome\".\n",
        "In case of delayed delivery except for Force Majeure cases, \"Dan\" (the Seller) shall pay to \"Steve\" (the Buyer) for every 2 days of delay penalty amounting to 10.5% of the total value of the Equipment whose delivery has been delayed. Any fractional part of a days is to be considered a full days. The total amount of penalty shall not however, exceed 55% of the total value of the Equipment involved in late delivery. If the delay is more than 15 days, the Buyer is entitled to terminate this Contract. All Equipment values are based on EUR and all penalty payments will be paid in USD at its equivalent amount in EUR.\n",
        "The conversion rate between the currencies is based upon \"the prevailing exchange rate at a major United States bank\".\n",
        "\"\"\"\n",
        "\n",
        "# Process the SLA text\n",
        "doc = nlp(sla_text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVGUJqWAOYuM",
        "outputId": "3b87e091-8007-4f38-bd74-9e1941e4044b"
      },
      "id": "gVGUJqWAOYuM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: The, POS: DET, Dependency: det\n",
            "Token: service, POS: NOUN, Dependency: compound\n",
            "Token: provider, POS: NOUN, Dependency: nsubj\n",
            "Token: shall, POS: AUX, Dependency: aux\n",
            "Token: ensure, POS: VERB, Dependency: ROOT\n",
            "Token: 99.9, POS: NUM, Dependency: nummod\n",
            "Token: %, POS: NOUN, Dependency: compound\n",
            "Token: uptime, POS: NOUN, Dependency: dobj\n",
            "Token: for, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Data, POS: PROPN, Dependency: compound\n",
            "Token: Backup, POS: PROPN, Dependency: compound\n",
            "Token: service, POS: NOUN, Dependency: pobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: In, POS: ADP, Dependency: prep\n",
            "Token: case, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: downtime, POS: NOUN, Dependency: pobj\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: response, POS: NOUN, Dependency: compound\n",
            "Token: time, POS: NOUN, Dependency: nsubj\n",
            "Token: should, POS: AUX, Dependency: aux\n",
            "Token: not, POS: PART, Dependency: neg\n",
            "Token: exceed, POS: VERB, Dependency: ROOT\n",
            "Token: 4, POS: NUM, Dependency: nummod\n",
            "Token: hours, POS: NOUN, Dependency: dobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: After, POS: ADP, Dependency: prep\n",
            "Token: execution, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: this, POS: DET, Dependency: det\n",
            "Token: Agreement, POS: NOUN, Dependency: pobj\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Dan, POS: PROPN, Dependency: nsubj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: shall, POS: AUX, Dependency: aux\n",
            "Token: pay, POS: VERB, Dependency: ROOT\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: full, POS: ADJ, Dependency: amod\n",
            "Token: purchase, POS: NOUN, Dependency: compound\n",
            "Token: price, POS: NOUN, Dependency: dobj\n",
            "Token: to, POS: ADP, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Jerome, POS: PROPN, Dependency: pobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: amount, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: 3.14, POS: NUM, Dependency: nummod\n",
            "Token: EUR, POS: PROPN, Dependency: pobj\n",
            "Token: upon, POS: SCONJ, Dependency: prep\n",
            "Token: demand, POS: NOUN, Dependency: pobj\n",
            "Token: by, POS: ADP, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Jerome, POS: PROPN, Dependency: pobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: In, POS: ADP, Dependency: prep\n",
            "Token: case, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: delayed, POS: VERB, Dependency: amod\n",
            "Token: delivery, POS: NOUN, Dependency: pobj\n",
            "Token: except, POS: SCONJ, Dependency: prep\n",
            "Token: for, POS: ADP, Dependency: prep\n",
            "Token: Force, POS: PROPN, Dependency: compound\n",
            "Token: Majeure, POS: PROPN, Dependency: compound\n",
            "Token: cases, POS: NOUN, Dependency: pobj\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Dan, POS: PROPN, Dependency: nsubj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: (, POS: PUNCT, Dependency: punct\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Seller, POS: PROPN, Dependency: appos\n",
            "Token: ), POS: PUNCT, Dependency: punct\n",
            "Token: shall, POS: AUX, Dependency: aux\n",
            "Token: pay, POS: VERB, Dependency: ROOT\n",
            "Token: to, POS: ADP, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Steve, POS: PROPN, Dependency: pobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: (, POS: PUNCT, Dependency: punct\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Buyer, POS: PROPN, Dependency: appos\n",
            "Token: ), POS: PUNCT, Dependency: punct\n",
            "Token: for, POS: ADP, Dependency: prep\n",
            "Token: every, POS: DET, Dependency: det\n",
            "Token: 2, POS: NUM, Dependency: nummod\n",
            "Token: days, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: delay, POS: NOUN, Dependency: compound\n",
            "Token: penalty, POS: NOUN, Dependency: pobj\n",
            "Token: amounting, POS: VERB, Dependency: advcl\n",
            "Token: to, POS: ADP, Dependency: prep\n",
            "Token: 10.5, POS: NUM, Dependency: nummod\n",
            "Token: %, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: total, POS: ADJ, Dependency: amod\n",
            "Token: value, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Equipment, POS: PROPN, Dependency: pobj\n",
            "Token: whose, POS: DET, Dependency: poss\n",
            "Token: delivery, POS: NOUN, Dependency: nsubjpass\n",
            "Token: has, POS: AUX, Dependency: aux\n",
            "Token: been, POS: AUX, Dependency: auxpass\n",
            "Token: delayed, POS: VERB, Dependency: relcl\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: Any, POS: DET, Dependency: det\n",
            "Token: fractional, POS: ADJ, Dependency: amod\n",
            "Token: part, POS: NOUN, Dependency: nsubj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: a, POS: DET, Dependency: det\n",
            "Token: days, POS: NOUN, Dependency: pobj\n",
            "Token: is, POS: AUX, Dependency: ROOT\n",
            "Token: to, POS: PART, Dependency: aux\n",
            "Token: be, POS: AUX, Dependency: auxpass\n",
            "Token: considered, POS: VERB, Dependency: xcomp\n",
            "Token: a, POS: DET, Dependency: det\n",
            "Token: full, POS: ADJ, Dependency: amod\n",
            "Token: days, POS: NOUN, Dependency: oprd\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: The, POS: DET, Dependency: det\n",
            "Token: total, POS: ADJ, Dependency: amod\n",
            "Token: amount, POS: NOUN, Dependency: nsubj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: penalty, POS: NOUN, Dependency: pobj\n",
            "Token: shall, POS: AUX, Dependency: aux\n",
            "Token: not, POS: PART, Dependency: neg\n",
            "Token: however, POS: ADV, Dependency: advmod\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: exceed, POS: VERB, Dependency: ROOT\n",
            "Token: 55, POS: NUM, Dependency: nummod\n",
            "Token: %, POS: NOUN, Dependency: dobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: total, POS: ADJ, Dependency: amod\n",
            "Token: value, POS: NOUN, Dependency: pobj\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Equipment, POS: NOUN, Dependency: pobj\n",
            "Token: involved, POS: VERB, Dependency: acl\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: late, POS: ADJ, Dependency: amod\n",
            "Token: delivery, POS: NOUN, Dependency: pobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: If, POS: SCONJ, Dependency: mark\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: delay, POS: NOUN, Dependency: nsubj\n",
            "Token: is, POS: AUX, Dependency: advcl\n",
            "Token: more, POS: ADJ, Dependency: amod\n",
            "Token: than, POS: ADP, Dependency: quantmod\n",
            "Token: 15, POS: NUM, Dependency: nummod\n",
            "Token: days, POS: NOUN, Dependency: attr\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Buyer, POS: PROPN, Dependency: nsubjpass\n",
            "Token: is, POS: AUX, Dependency: auxpass\n",
            "Token: entitled, POS: VERB, Dependency: ROOT\n",
            "Token: to, POS: PART, Dependency: aux\n",
            "Token: terminate, POS: VERB, Dependency: xcomp\n",
            "Token: this, POS: DET, Dependency: det\n",
            "Token: Contract, POS: PROPN, Dependency: dobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: All, POS: DET, Dependency: det\n",
            "Token: Equipment, POS: PROPN, Dependency: compound\n",
            "Token: values, POS: NOUN, Dependency: nsubjpass\n",
            "Token: are, POS: AUX, Dependency: auxpass\n",
            "Token: based, POS: VERB, Dependency: ROOT\n",
            "Token: on, POS: ADP, Dependency: prep\n",
            "Token: EUR, POS: PROPN, Dependency: pobj\n",
            "Token: and, POS: CCONJ, Dependency: cc\n",
            "Token: all, POS: DET, Dependency: det\n",
            "Token: penalty, POS: NOUN, Dependency: compound\n",
            "Token: payments, POS: NOUN, Dependency: conj\n",
            "Token: will, POS: AUX, Dependency: aux\n",
            "Token: be, POS: AUX, Dependency: auxpass\n",
            "Token: paid, POS: VERB, Dependency: conj\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: USD, POS: NOUN, Dependency: pobj\n",
            "Token: at, POS: ADP, Dependency: prep\n",
            "Token: its, POS: PRON, Dependency: poss\n",
            "Token: equivalent, POS: ADJ, Dependency: amod\n",
            "Token: amount, POS: NOUN, Dependency: pobj\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: EUR, POS: PROPN, Dependency: pobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: The, POS: DET, Dependency: det\n",
            "Token: conversion, POS: NOUN, Dependency: compound\n",
            "Token: rate, POS: NOUN, Dependency: nsubjpass\n",
            "Token: between, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: currencies, POS: NOUN, Dependency: pobj\n",
            "Token: is, POS: AUX, Dependency: auxpass\n",
            "Token: based, POS: VERB, Dependency: ROOT\n",
            "Token: upon, POS: SCONJ, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: prevailing, POS: VERB, Dependency: amod\n",
            "Token: exchange, POS: NOUN, Dependency: compound\n",
            "Token: rate, POS: NOUN, Dependency: pobj\n",
            "Token: at, POS: ADP, Dependency: prep\n",
            "Token: a, POS: DET, Dependency: det\n",
            "Token: major, POS: ADJ, Dependency: amod\n",
            "Token: United, POS: PROPN, Dependency: compound\n",
            "Token: States, POS: PROPN, Dependency: compound\n",
            "Token: bank, POS: NOUN, Dependency: pobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Entity: 99.9%, Label: PERCENT\n",
            "Entity: Data Backup, Label: ORG\n",
            "Entity: 4 hours, Label: TIME\n",
            "Entity: Dan, Label: PERSON\n",
            "Entity: Jerome, Label: WORK_OF_ART\n",
            "Entity: 3.14, Label: CARDINAL\n",
            "Entity: EUR, Label: ORG\n",
            "Entity: Jerome, Label: PERSON\n",
            "Entity: Force Majeure, Label: ORG\n",
            "Entity: Dan, Label: PERSON\n",
            "Entity: Seller, Label: PERSON\n",
            "Entity: Steve, Label: PERSON\n",
            "Entity: Buyer, Label: PERSON\n",
            "Entity: every 2 days, Label: DATE\n",
            "Entity: 10.5%, Label: PERCENT\n",
            "Entity: a full days, Label: DATE\n",
            "Entity: 55%, Label: PERCENT\n",
            "Entity: more than 15 days, Label: DATE\n",
            "Entity: Buyer, Label: PERSON\n",
            "Entity: EUR, Label: ORG\n",
            "Entity: USD, Label: ORG\n",
            "Entity: EUR, Label: ORG\n",
            "Entity: United States, Label: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {\"latency\", \"throughput\", \"reliability\", \"availability\", \"jitter\", \"packet\", \"loss\", \"qos\", \"acceptance\", \"notifies\", \"Business Days\", \"requirements\", \"obligations\", \"agreement\" }\n",
        "\n",
        "for token in doc:\n",
        "    if token.text.lower() in keywords:\n",
        "        print(f\"\\nKeyword: {token.text}\")\n",
        "        for child in token.children:\n",
        "            print(f\"  -> Child: {child.text} ({child.dep_}, {child.pos_})\")\n",
        "        for ancestor in token.ancestors:\n",
        "            print(f\"  <- Ancestor: {ancestor.text} ({ancestor.dep_}, {ancestor.pos_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye2GYF_iSJoK",
        "outputId": "1c4ee967-aa5a-4df8-e8f0-78dbc4e88e4b"
      },
      "id": "ye2GYF_iSJoK",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Keyword: Agreement\n",
            "  -> Child: this (det, DET)\n",
            "  <- Ancestor: of (prep, ADP)\n",
            "  <- Ancestor: execution (pobj, NOUN)\n",
            "  <- Ancestor: After (prep, ADP)\n",
            "  <- Ancestor: pay (ROOT, VERB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbNvxRJzSRfa",
        "outputId": "219b2d0d-2a85-4ea3-f2e3-54239080ef3f"
      },
      "id": "nbNvxRJzSRfa",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: 99.9%, Label: PERCENT\n",
            "Entity: Data Backup, Label: ORG\n",
            "Entity: 4 hours, Label: TIME\n",
            "Entity: Dan, Label: PERSON\n",
            "Entity: Jerome, Label: WORK_OF_ART\n",
            "Entity: 3.14, Label: CARDINAL\n",
            "Entity: EUR, Label: ORG\n",
            "Entity: Jerome, Label: PERSON\n",
            "Entity: Force Majeure, Label: ORG\n",
            "Entity: Dan, Label: PERSON\n",
            "Entity: Seller, Label: PERSON\n",
            "Entity: Steve, Label: PERSON\n",
            "Entity: Buyer, Label: PERSON\n",
            "Entity: every 2 days, Label: DATE\n",
            "Entity: 10.5%, Label: PERCENT\n",
            "Entity: a full days, Label: DATE\n",
            "Entity: 55%, Label: PERCENT\n",
            "Entity: more than 15 days, Label: DATE\n",
            "Entity: Buyer, Label: PERSON\n",
            "Entity: EUR, Label: ORG\n",
            "Entity: USD, Label: ORG\n",
            "Entity: EUR, Label: ORG\n",
            "Entity: United States, Label: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use spaCy's Matcher to define patterns that match\n",
        "#specific phrases or structures within the text, such as service names, availability percentages, and response times.\n",
        "\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Initialize the matcher with the shared vocabulary\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define patterns for availability and response time\n",
        "availability_pattern = [{'LIKE_NUM': True}, {'TEXT': '%'}, {'LOWER': 'uptime'}]\n",
        "response_time_pattern = [{'LOWER': 'response'}, {'LOWER': 'time'}, {'LOWER': 'should'}, {'LOWER': 'not'}, {'LOWER': 'exceed'}, {'LIKE_NUM': True}, {'LOWER': 'hours'}]\n",
        "\n",
        "# Add patterns to the matcher\n",
        "matcher.add('AVAILABILITY', [availability_pattern])\n",
        "matcher.add('RESPONSE_TIME', [response_time_pattern])\n",
        "\n",
        "# Apply the matcher to the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Initialize variables\n",
        "availability = None\n",
        "response_time = None\n",
        "\n",
        "# Extract matched spans\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    match_label = nlp.vocab.strings[match_id]\n",
        "    if match_label == 'AVAILABILITY':\n",
        "        availability = span.text\n",
        "    elif match_label == 'RESPONSE_TIME':\n",
        "        response_time = span.text\n",
        "\n",
        "print(f\"Availability: {availability}\")\n",
        "print(f\"Response Time: {response_time}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B2kqaGZOZoB",
        "outputId": "db819cf7-c3d2-479a-fb48-803a568cdc1c"
      },
      "id": "3B2kqaGZOZoB",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Availability: 99.9% uptime\n",
            "Response Time: response time should not exceed 4 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Existing patterns\n",
        "availability_pattern = [{'LIKE_NUM': True}, {'TEXT': '%'}, {'LOWER': 'uptime'}]\n",
        "response_time_pattern = [{'LOWER': 'response'}, {'LOWER': 'time'}, {'LOWER': 'should'}, {'LOWER': 'not'}, {'LOWER': 'exceed'}, {'LIKE_NUM': True}, {'LOWER': 'hours'}]\n",
        "\n",
        "matcher.add('AVAILABILITY', [availability_pattern])\n",
        "matcher.add('RESPONSE_TIME', [response_time_pattern])\n",
        "\n",
        "# New keyword patterns\n",
        "keywords = [\n",
        "    \"latency\", \"throughput\", \"reliability\", \"availability\", \"jitter\",\n",
        "    \"packet\", \"loss\", \"qos\", \"acceptance\", \"notifies\", \"name\", \"license\", \"agreement\", \"sla\", \"licensor\", \"company\",\n",
        "    \"business days\", \"requirements\", \"obligations\", \"usd\", \"dollars\", \"date\"\n",
        "]\n",
        "\n",
        "for keyword in keywords:\n",
        "    words = keyword.lower().split()\n",
        "    pattern = [{'LOWER': word.strip(\",\")} for word in words]\n",
        "    matcher.add(keyword.upper().replace(\" \", \"_\"), [pattern])\n",
        "\n",
        "# Apply the matcher\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Print matches\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(f\"Matched '{span.text}' with label '{nlp.vocab.strings[match_id]}' at position {start}-{end}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5FkfvL5TiSo",
        "outputId": "f7260852-b275-4e34-c538-eb69f0f8adc6"
      },
      "id": "o5FkfvL5TiSo",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched '99.9% uptime' with label 'AVAILABILITY' at position 6-9\n",
            "Matched 'response time should not exceed 4 hours' with label 'RESPONSE_TIME' at position 22-29\n",
            "Matched 'Agreement' with label 'AGREEMENT' at position 35-36\n",
            "Matched 'USD' with label 'USD' at position 187-188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: define custom overrides for certain proper nouns\n",
        "custom_lemma_map = {\n",
        "    \"Licensor\": \"license\",\n",
        "    \"Licensee\": \"license\"\n",
        "}\n",
        "\n",
        "# Print tokens and their lemmas\n",
        "for token in doc:\n",
        "    if token.text in custom_lemma_map:\n",
        "        lemma = custom_lemma_map[token.text]\n",
        "    else:\n",
        "        lemma = token.lemma_\n",
        "    print(f\"Token: {token.text} -> Lemma: {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp_iyA6xVK9H",
        "outputId": "84109a04-31cf-473d-c1fa-ef3afbd2889f"
      },
      "id": "Gp_iyA6xVK9H",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: The -> Lemma: the\n",
            "Token: service -> Lemma: service\n",
            "Token: provider -> Lemma: provider\n",
            "Token: shall -> Lemma: shall\n",
            "Token: ensure -> Lemma: ensure\n",
            "Token: 99.9 -> Lemma: 99.9\n",
            "Token: % -> Lemma: %\n",
            "Token: uptime -> Lemma: uptime\n",
            "Token: for -> Lemma: for\n",
            "Token: the -> Lemma: the\n",
            "Token: Data -> Lemma: Data\n",
            "Token: Backup -> Lemma: Backup\n",
            "Token: service -> Lemma: service\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: In -> Lemma: in\n",
            "Token: case -> Lemma: case\n",
            "Token: of -> Lemma: of\n",
            "Token: downtime -> Lemma: downtime\n",
            "Token: , -> Lemma: ,\n",
            "Token: the -> Lemma: the\n",
            "Token: response -> Lemma: response\n",
            "Token: time -> Lemma: time\n",
            "Token: should -> Lemma: should\n",
            "Token: not -> Lemma: not\n",
            "Token: exceed -> Lemma: exceed\n",
            "Token: 4 -> Lemma: 4\n",
            "Token: hours -> Lemma: hour\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: After -> Lemma: after\n",
            "Token: execution -> Lemma: execution\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Dan -> Lemma: Dan\n",
            "Token: \" -> Lemma: \"\n",
            "Token: shall -> Lemma: shall\n",
            "Token: pay -> Lemma: pay\n",
            "Token: the -> Lemma: the\n",
            "Token: full -> Lemma: full\n",
            "Token: purchase -> Lemma: purchase\n",
            "Token: price -> Lemma: price\n",
            "Token: to -> Lemma: to\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Jerome -> Lemma: Jerome\n",
            "Token: \" -> Lemma: \"\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: amount -> Lemma: amount\n",
            "Token: of -> Lemma: of\n",
            "Token: 3.14 -> Lemma: 3.14\n",
            "Token: EUR -> Lemma: EUR\n",
            "Token: upon -> Lemma: upon\n",
            "Token: demand -> Lemma: demand\n",
            "Token: by -> Lemma: by\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Jerome -> Lemma: Jerome\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: In -> Lemma: in\n",
            "Token: case -> Lemma: case\n",
            "Token: of -> Lemma: of\n",
            "Token: delayed -> Lemma: delay\n",
            "Token: delivery -> Lemma: delivery\n",
            "Token: except -> Lemma: except\n",
            "Token: for -> Lemma: for\n",
            "Token: Force -> Lemma: Force\n",
            "Token: Majeure -> Lemma: Majeure\n",
            "Token: cases -> Lemma: case\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Dan -> Lemma: Dan\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: Seller -> Lemma: Seller\n",
            "Token: ) -> Lemma: )\n",
            "Token: shall -> Lemma: shall\n",
            "Token: pay -> Lemma: pay\n",
            "Token: to -> Lemma: to\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Steve -> Lemma: Steve\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: Buyer -> Lemma: Buyer\n",
            "Token: ) -> Lemma: )\n",
            "Token: for -> Lemma: for\n",
            "Token: every -> Lemma: every\n",
            "Token: 2 -> Lemma: 2\n",
            "Token: days -> Lemma: day\n",
            "Token: of -> Lemma: of\n",
            "Token: delay -> Lemma: delay\n",
            "Token: penalty -> Lemma: penalty\n",
            "Token: amounting -> Lemma: amount\n",
            "Token: to -> Lemma: to\n",
            "Token: 10.5 -> Lemma: 10.5\n",
            "Token: % -> Lemma: %\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: total -> Lemma: total\n",
            "Token: value -> Lemma: value\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: Equipment -> Lemma: Equipment\n",
            "Token: whose -> Lemma: whose\n",
            "Token: delivery -> Lemma: delivery\n",
            "Token: has -> Lemma: have\n",
            "Token: been -> Lemma: be\n",
            "Token: delayed -> Lemma: delay\n",
            "Token: . -> Lemma: .\n",
            "Token: Any -> Lemma: any\n",
            "Token: fractional -> Lemma: fractional\n",
            "Token: part -> Lemma: part\n",
            "Token: of -> Lemma: of\n",
            "Token: a -> Lemma: a\n",
            "Token: days -> Lemma: day\n",
            "Token: is -> Lemma: be\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: considered -> Lemma: consider\n",
            "Token: a -> Lemma: a\n",
            "Token: full -> Lemma: full\n",
            "Token: days -> Lemma: day\n",
            "Token: . -> Lemma: .\n",
            "Token: The -> Lemma: the\n",
            "Token: total -> Lemma: total\n",
            "Token: amount -> Lemma: amount\n",
            "Token: of -> Lemma: of\n",
            "Token: penalty -> Lemma: penalty\n",
            "Token: shall -> Lemma: shall\n",
            "Token: not -> Lemma: not\n",
            "Token: however -> Lemma: however\n",
            "Token: , -> Lemma: ,\n",
            "Token: exceed -> Lemma: exceed\n",
            "Token: 55 -> Lemma: 55\n",
            "Token: % -> Lemma: %\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: total -> Lemma: total\n",
            "Token: value -> Lemma: value\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: Equipment -> Lemma: equipment\n",
            "Token: involved -> Lemma: involve\n",
            "Token: in -> Lemma: in\n",
            "Token: late -> Lemma: late\n",
            "Token: delivery -> Lemma: delivery\n",
            "Token: . -> Lemma: .\n",
            "Token: If -> Lemma: if\n",
            "Token: the -> Lemma: the\n",
            "Token: delay -> Lemma: delay\n",
            "Token: is -> Lemma: be\n",
            "Token: more -> Lemma: more\n",
            "Token: than -> Lemma: than\n",
            "Token: 15 -> Lemma: 15\n",
            "Token: days -> Lemma: day\n",
            "Token: , -> Lemma: ,\n",
            "Token: the -> Lemma: the\n",
            "Token: Buyer -> Lemma: Buyer\n",
            "Token: is -> Lemma: be\n",
            "Token: entitled -> Lemma: entitle\n",
            "Token: to -> Lemma: to\n",
            "Token: terminate -> Lemma: terminate\n",
            "Token: this -> Lemma: this\n",
            "Token: Contract -> Lemma: Contract\n",
            "Token: . -> Lemma: .\n",
            "Token: All -> Lemma: all\n",
            "Token: Equipment -> Lemma: Equipment\n",
            "Token: values -> Lemma: value\n",
            "Token: are -> Lemma: be\n",
            "Token: based -> Lemma: base\n",
            "Token: on -> Lemma: on\n",
            "Token: EUR -> Lemma: EUR\n",
            "Token: and -> Lemma: and\n",
            "Token: all -> Lemma: all\n",
            "Token: penalty -> Lemma: penalty\n",
            "Token: payments -> Lemma: payment\n",
            "Token: will -> Lemma: will\n",
            "Token: be -> Lemma: be\n",
            "Token: paid -> Lemma: pay\n",
            "Token: in -> Lemma: in\n",
            "Token: USD -> Lemma: usd\n",
            "Token: at -> Lemma: at\n",
            "Token: its -> Lemma: its\n",
            "Token: equivalent -> Lemma: equivalent\n",
            "Token: amount -> Lemma: amount\n",
            "Token: in -> Lemma: in\n",
            "Token: EUR -> Lemma: EUR\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: The -> Lemma: the\n",
            "Token: conversion -> Lemma: conversion\n",
            "Token: rate -> Lemma: rate\n",
            "Token: between -> Lemma: between\n",
            "Token: the -> Lemma: the\n",
            "Token: currencies -> Lemma: currency\n",
            "Token: is -> Lemma: be\n",
            "Token: based -> Lemma: base\n",
            "Token: upon -> Lemma: upon\n",
            "Token: \" -> Lemma: \"\n",
            "Token: the -> Lemma: the\n",
            "Token: prevailing -> Lemma: prevail\n",
            "Token: exchange -> Lemma: exchange\n",
            "Token: rate -> Lemma: rate\n",
            "Token: at -> Lemma: at\n",
            "Token: a -> Lemma: a\n",
            "Token: major -> Lemma: major\n",
            "Token: United -> Lemma: United\n",
            "Token: States -> Lemma: States\n",
            "Token: bank -> Lemma: bank\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure the extracted information into an XML format using Python's xml.etree.ElementTree module.\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Create the root element\n",
        "sla = ET.Element('SLA')\n",
        "\n",
        "# Create a service element\n",
        "service = ET.SubElement(sla, 'Service')\n",
        "\n",
        "# Add service name\n",
        "service_name = ET.SubElement(service, 'Name')\n",
        "service_name.text = 'Data Backup'  # This can be extracted similarly using spaCy\n",
        "\n",
        "# Add availability\n",
        "if availability:\n",
        "    availability_elem = ET.SubElement(service, 'Availability')\n",
        "    availability_elem.text = availability\n",
        "\n",
        "# Add response time\n",
        "if response_time:\n",
        "    response_time_elem = ET.SubElement(service, 'ResponseTime')\n",
        "    response_time_elem.text = response_time\n",
        "\n",
        "# Generate the XML string\n",
        "xml_str = ET.tostring(sla, encoding='unicode')\n",
        "print(xml_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "244uhX2qOdiy",
        "outputId": "dbf9149a-dc15-4d6b-d0d0-07d7d866e450"
      },
      "id": "244uhX2qOdiy",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SLA><Service><Name>Data Backup</Name><Availability>99.9% uptime</Availability><ResponseTime>response time should not exceed 4 hours</ResponseTime></Service></SLA>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming xml_str contains your XML content as a string\n",
        "with open('slaexample_output.xml', 'w', encoding='utf-8') as f:\n",
        "    f.write(xml_str)\n"
      ],
      "metadata": {
        "id": "3VbhYGAcOq05"
      },
      "id": "3VbhYGAcOq05",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Assuming 'sla' is your root Element\n",
        "tree = ET.ElementTree(sla)\n",
        "tree.write('slaexample_output.xml', encoding='utf-8', xml_declaration=True)\n"
      ],
      "metadata": {
        "id": "z1woSwYzP-Pu"
      },
      "id": "z1woSwYzP-Pu",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spacy on accord project**"
      ],
      "metadata": {
        "id": "ycTI2SIKxtRD"
      },
      "id": "ycTI2SIKxtRD"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load('en_core_web_sm')  # You can choose a different model as needed\n",
        "\n",
        "\n",
        "sla_text = \"\"\"\n",
        "Acceptance of Delivery.\n",
        "\"Party A\" will be deemed to have completed its delivery obligations if in \"Party B\"'s opinion, the \"Widgets\" satisfies the Acceptance Criteria, and \"Party B\" notifies \"Party A\" in writing that it is accepting the \"Widgets\".\n",
        "\n",
        "Inspection and Notice.\n",
        "\"Party B\" will have 10 Business Days to inspect and evaluate the \"Widgets\" on the delivery date before notifying \"Party A\" that it is either accepting or rejecting the \"Widgets\".\n",
        "\n",
        "Acceptance Criteria.\n",
        "The \"Acceptance Criteria\" are the specifications the \"Widgets\" must meet for \"Party A\" to comply with its requirements and obligations under this agreement, detailed in \"Attachment X\", attached to this agreement.\n",
        "\"\"\"\n",
        "\n",
        "# Process the SLA text\n",
        "doc = nlp(sla_text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "id": "aNHJdpsSQAkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddea332e-61be-44b1-b894-a8c0e26a8e01"
      },
      "id": "aNHJdpsSQAkA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: Acceptance, POS: PROPN, Dependency: ROOT\n",
            "Token: of, POS: ADP, Dependency: prep\n",
            "Token: Delivery, POS: PROPN, Dependency: pobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: compound\n",
            "Token: A, POS: PROPN, Dependency: nsubjpass\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: will, POS: AUX, Dependency: aux\n",
            "Token: be, POS: AUX, Dependency: auxpass\n",
            "Token: deemed, POS: VERB, Dependency: ccomp\n",
            "Token: to, POS: PART, Dependency: aux\n",
            "Token: have, POS: AUX, Dependency: aux\n",
            "Token: completed, POS: VERB, Dependency: xcomp\n",
            "Token: its, POS: PRON, Dependency: poss\n",
            "Token: delivery, POS: NOUN, Dependency: compound\n",
            "Token: obligations, POS: NOUN, Dependency: dobj\n",
            "Token: if, POS: SCONJ, Dependency: mark\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: compound\n",
            "Token: B, POS: PROPN, Dependency: poss\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: 's, POS: PART, Dependency: case\n",
            "Token: opinion, POS: NOUN, Dependency: pobj\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Widgets, POS: NOUN, Dependency: nsubj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: satisfies, POS: VERB, Dependency: ROOT\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: Acceptance, POS: PROPN, Dependency: compound\n",
            "Token: Criteria, POS: PROPN, Dependency: dobj\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: and, POS: CCONJ, Dependency: cc\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: nmod\n",
            "Token: B, POS: PROPN, Dependency: nmod\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: notifies, POS: NOUN, Dependency: conj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: compound\n",
            "Token: A, POS: PROPN, Dependency: appos\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: writing, POS: VERB, Dependency: pcomp\n",
            "Token: that, POS: SCONJ, Dependency: mark\n",
            "Token: it, POS: PRON, Dependency: nsubj\n",
            "Token: is, POS: AUX, Dependency: aux\n",
            "Token: accepting, POS: VERB, Dependency: ccomp\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Widgets, POS: NOUN, Dependency: dobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: Inspection, POS: PROPN, Dependency: ROOT\n",
            "Token: and, POS: CCONJ, Dependency: cc\n",
            "Token: Notice, POS: PROPN, Dependency: conj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: compound\n",
            "Token: B, POS: PROPN, Dependency: nsubj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: will, POS: AUX, Dependency: aux\n",
            "Token: have, POS: VERB, Dependency: ROOT\n",
            "Token: 10, POS: NUM, Dependency: nummod\n",
            "Token: Business, POS: PROPN, Dependency: compound\n",
            "Token: Days, POS: NOUN, Dependency: dobj\n",
            "Token: to, POS: PART, Dependency: aux\n",
            "Token: inspect, POS: VERB, Dependency: relcl\n",
            "Token: and, POS: CCONJ, Dependency: cc\n",
            "Token: evaluate, POS: VERB, Dependency: conj\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Widgets, POS: NOUN, Dependency: dobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: on, POS: ADP, Dependency: prep\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: delivery, POS: NOUN, Dependency: compound\n",
            "Token: date, POS: NOUN, Dependency: pobj\n",
            "Token: before, POS: ADP, Dependency: prep\n",
            "Token: notifying, POS: VERB, Dependency: pcomp\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: compound\n",
            "Token: A, POS: PROPN, Dependency: dobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: that, POS: SCONJ, Dependency: mark\n",
            "Token: it, POS: PRON, Dependency: nsubj\n",
            "Token: is, POS: AUX, Dependency: aux\n",
            "Token: either, POS: CCONJ, Dependency: preconj\n",
            "Token: accepting, POS: VERB, Dependency: ccomp\n",
            "Token: or, POS: CCONJ, Dependency: cc\n",
            "Token: rejecting, POS: VERB, Dependency: conj\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Widgets, POS: NOUN, Dependency: dobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: Acceptance, POS: PROPN, Dependency: compound\n",
            "Token: Criteria, POS: PROPN, Dependency: pobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Token: The, POS: DET, Dependency: det\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Acceptance, POS: PROPN, Dependency: compound\n",
            "Token: Criteria, POS: PROPN, Dependency: nsubj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: are, POS: AUX, Dependency: ROOT\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: specifications, POS: NOUN, Dependency: attr\n",
            "Token: the, POS: DET, Dependency: det\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Widgets, POS: NOUN, Dependency: nsubj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: must, POS: AUX, Dependency: aux\n",
            "Token: meet, POS: VERB, Dependency: relcl\n",
            "Token: for, POS: ADP, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Party, POS: PROPN, Dependency: compound\n",
            "Token: A, POS: PROPN, Dependency: pobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: to, POS: PART, Dependency: aux\n",
            "Token: comply, POS: VERB, Dependency: advcl\n",
            "Token: with, POS: ADP, Dependency: prep\n",
            "Token: its, POS: PRON, Dependency: poss\n",
            "Token: requirements, POS: NOUN, Dependency: pobj\n",
            "Token: and, POS: CCONJ, Dependency: cc\n",
            "Token: obligations, POS: NOUN, Dependency: conj\n",
            "Token: under, POS: ADP, Dependency: prep\n",
            "Token: this, POS: DET, Dependency: det\n",
            "Token: agreement, POS: NOUN, Dependency: pobj\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: detailed, POS: VERB, Dependency: advcl\n",
            "Token: in, POS: ADP, Dependency: prep\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: Attachment, POS: PROPN, Dependency: compound\n",
            "Token: X, POS: PROPN, Dependency: pobj\n",
            "Token: \", POS: PUNCT, Dependency: punct\n",
            "Token: ,, POS: PUNCT, Dependency: punct\n",
            "Token: attached, POS: VERB, Dependency: advcl\n",
            "Token: to, POS: ADP, Dependency: prep\n",
            "Token: this, POS: DET, Dependency: det\n",
            "Token: agreement, POS: NOUN, Dependency: pobj\n",
            "Token: ., POS: PUNCT, Dependency: punct\n",
            "Token: \n",
            ", POS: SPACE, Dependency: dep\n",
            "Entity: the Acceptance Criteria, Label: ORG\n",
            "Entity: Notice, Label: ORG\n",
            "Entity: 10 Business Days, Label: DATE\n",
            "Entity: Party A, Label: WORK_OF_ART\n",
            "Entity: The \"Acceptance Criteria, Label: WORK_OF_ART\n",
            "Entity: Attachment X, Label: WORK_OF_ART\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {\"latency\", \"throughput\", \"reliability\", \"availability\", \"jitter\", \"packet\", \"loss\", \"qos\", \"acceptance\", \"notifies\", \"Business Days\", \"requirements\", \"obligations\", \"agreement\" }\n",
        "\n",
        "for token in doc:\n",
        "    if token.text.lower() in keywords:\n",
        "        print(f\"\\nKeyword: {token.text}\")\n",
        "        for child in token.children:\n",
        "            print(f\"  -> Child: {child.text} ({child.dep_}, {child.pos_})\")\n",
        "        for ancestor in token.ancestors:\n",
        "            print(f\"  <- Ancestor: {ancestor.text} ({ancestor.dep_}, {ancestor.pos_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67V3DphxyEcf",
        "outputId": "bfce6607-58bb-479a-a10a-93b3bd0d5243"
      },
      "id": "67V3DphxyEcf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Keyword: Acceptance\n",
            "  -> Child: \n",
            " (dep, SPACE)\n",
            "  -> Child: of (prep, ADP)\n",
            "  -> Child: . (punct, PUNCT)\n",
            "\n",
            "Keyword: obligations\n",
            "  -> Child: its (poss, PRON)\n",
            "  -> Child: delivery (compound, NOUN)\n",
            "  <- Ancestor: completed (xcomp, VERB)\n",
            "  <- Ancestor: deemed (ccomp, VERB)\n",
            "  <- Ancestor: satisfies (ROOT, VERB)\n",
            "\n",
            "Keyword: Acceptance\n",
            "  <- Ancestor: Criteria (dobj, PROPN)\n",
            "  <- Ancestor: satisfies (ROOT, VERB)\n",
            "\n",
            "Keyword: notifies\n",
            "  -> Child: \" (punct, PUNCT)\n",
            "  -> Child: B (nmod, PROPN)\n",
            "  -> Child: \" (punct, PUNCT)\n",
            "  -> Child: \" (punct, PUNCT)\n",
            "  -> Child: A (appos, PROPN)\n",
            "  -> Child: \" (punct, PUNCT)\n",
            "  <- Ancestor: satisfies (ROOT, VERB)\n",
            "\n",
            "Keyword: Acceptance\n",
            "  <- Ancestor: Criteria (pobj, PROPN)\n",
            "  <- Ancestor: . (punct, PUNCT)\n",
            "  <- Ancestor: have (ROOT, VERB)\n",
            "\n",
            "Keyword: Acceptance\n",
            "  <- Ancestor: Criteria (nsubj, PROPN)\n",
            "  <- Ancestor: are (ROOT, AUX)\n",
            "\n",
            "Keyword: requirements\n",
            "  -> Child: its (poss, PRON)\n",
            "  -> Child: and (cc, CCONJ)\n",
            "  -> Child: obligations (conj, NOUN)\n",
            "  -> Child: under (prep, ADP)\n",
            "  <- Ancestor: with (prep, ADP)\n",
            "  <- Ancestor: comply (advcl, VERB)\n",
            "  <- Ancestor: meet (relcl, VERB)\n",
            "  <- Ancestor: specifications (attr, NOUN)\n",
            "  <- Ancestor: are (ROOT, AUX)\n",
            "\n",
            "Keyword: obligations\n",
            "  <- Ancestor: requirements (pobj, NOUN)\n",
            "  <- Ancestor: with (prep, ADP)\n",
            "  <- Ancestor: comply (advcl, VERB)\n",
            "  <- Ancestor: meet (relcl, VERB)\n",
            "  <- Ancestor: specifications (attr, NOUN)\n",
            "  <- Ancestor: are (ROOT, AUX)\n",
            "\n",
            "Keyword: agreement\n",
            "  -> Child: this (det, DET)\n",
            "  <- Ancestor: under (prep, ADP)\n",
            "  <- Ancestor: requirements (pobj, NOUN)\n",
            "  <- Ancestor: with (prep, ADP)\n",
            "  <- Ancestor: comply (advcl, VERB)\n",
            "  <- Ancestor: meet (relcl, VERB)\n",
            "  <- Ancestor: specifications (attr, NOUN)\n",
            "  <- Ancestor: are (ROOT, AUX)\n",
            "\n",
            "Keyword: agreement\n",
            "  -> Child: this (det, DET)\n",
            "  <- Ancestor: to (prep, ADP)\n",
            "  <- Ancestor: attached (advcl, VERB)\n",
            "  <- Ancestor: are (ROOT, AUX)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcUgCvAMywxd",
        "outputId": "0bbb429b-8f59-4957-921e-ebecc15c57c6"
      },
      "id": "QcUgCvAMywxd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: the Acceptance Criteria, Label: ORG\n",
            "Entity: Notice, Label: ORG\n",
            "Entity: 10 Business Days, Label: DATE\n",
            "Entity: Party A, Label: WORK_OF_ART\n",
            "Entity: The \"Acceptance Criteria, Label: WORK_OF_ART\n",
            "Entity: Attachment X, Label: WORK_OF_ART\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "terms = [\"service uptime\", \"maximum\", \"delivery\", \"service\"]\n",
        "patterns = [nlp.make_doc(text) for text in terms]\n",
        "matcher.add(\"SLA_TERMS\", patterns)\n",
        "\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(f\"Matched phrase: {span.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKT7tRkXzn_g",
        "outputId": "140f37fb-3327-4863-fe63-98fa42d987d9"
      },
      "id": "OKT7tRkXzn_g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched phrase: delivery\n",
            "Matched phrase: delivery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **large English NLP model provided by spaCy**"
      ],
      "metadata": {
        "id": "r3HtupNZ44aC"
      },
      "id": "r3HtupNZ44aC"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igNwdz840HHP",
        "outputId": "632aed8c-415c-4723-f535-df45538b003b"
      },
      "id": "igNwdz840HHP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp = spacy.load('en_core_web_lg')  # Must use a large model with word vectors\n",
        "# target = nlp(\"latency\")[0]\n",
        "\n",
        "# for token in doc:\n",
        "#     if token.has_vector and token.similarity(target) > 0.6:\n",
        "#         print(f\"Related to latency: {token.text}, Similarity: {token.similarity(target):.2f}\")\n",
        "\n",
        "# Step 3: Now your similarity code will work\n",
        "doc = nlp(\"The latency must be below 5 milliseconds to meet SLA.\")\n",
        "target = nlp(\"latency\")[0]\n",
        "\n",
        "for token in doc:\n",
        "    if token.has_vector and token.similarity(target) > 0.6:\n",
        "        print(f\"Related to latency: {token.text}, Similarity: {token.similarity(target):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnQBxKHfzzSy",
        "outputId": "939d0f73-8ae1-4247-b39a-9e07b34eff52"
      },
      "id": "lnQBxKHfzzSy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Related to latency: latency, Similarity: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the large English model with word vectors\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# SLA-related keywords to compare against\n",
        "keywords = [\n",
        "    \"latency\", \"throughput\", \"reliability\", \"availability\", \"jitter\",\n",
        "    \"packet\", \"loss\", \"qos\", \"acceptance\", \"notifies\",\n",
        "    \"Business Days\", \"requirements\", \"obligations\", \"agreement\"\n",
        "]\n",
        "\n",
        "# Convert keywords to spaCy tokens (handle phrases properly)\n",
        "keyword_tokens = {kw: nlp(kw)[0] if len(nlp(kw)) == 1 else nlp(kw) for kw in keywords}\n",
        "\n",
        "# Text to process\n",
        "doc = nlp(\"\"\"\n",
        "# The latency must be below 5 milliseconds to meet SLA.\n",
        "# Party B will have 10 Business Days to evaluate the service availability and notify Party A.\n",
        "Acceptance of Delivery.\n",
        "\"Party A\" will be deemed to have completed its delivery obligations if in \"Party B\"'s opinion, the \"Widgets\" satisfies the Acceptance Criteria, and \"Party B\" notifies \"Party A\" in writing that it is accepting the \"Widgets\".\n",
        "\n",
        "Inspection and Notice.\n",
        "\"Party B\" will have 10 Business Days to inspect and evaluate the \"Widgets\" on the delivery date before notifying \"Party A\" that it is either accepting or rejecting the \"Widgets\".\n",
        "\n",
        "Acceptance Criteria.\n",
        "The \"Acceptance Criteria\" are the specifications the \"Widgets\" must meet for \"Party A\" to comply with its requirements and obligations under this agreement, detailed in \"Attachment X\", attached to this agreement.\n",
        "\"\"\")\n",
        "\n",
        "# Compare each token in the doc to each keyword\n",
        "for token in doc:\n",
        "    if token.has_vector:\n",
        "        for kw, kw_token in keyword_tokens.items():\n",
        "            if isinstance(kw_token, spacy.tokens.Token):\n",
        "                # Single-word keyword\n",
        "                similarity = token.similarity(kw_token)\n",
        "            else:\n",
        "                # Multi-word keyword (e.g., \"Business Days\") ‚Äî average similarity\n",
        "                similarity = sum(token.similarity(t) for t in kw_token) / len(kw_token)\n",
        "\n",
        "            if similarity > 0.6:\n",
        "                print(f\"Doc token '{token.text}' is similar to keyword '{kw}' (Similarity: {similarity:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LFhLSY_z2e6",
        "outputId": "ee22467b-e650-4147-f38b-ed6f4729ee96"
      },
      "id": "0LFhLSY_z2e6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc token 'latency' is similar to keyword 'latency' (Similarity: 1.00)\n",
            "Doc token 'latency' is similar to keyword 'throughput' (Similarity: 0.63)\n",
            "Doc token 'latency' is similar to keyword 'jitter' (Similarity: 0.65)\n",
            "Doc token 'Business' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'Days' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'availability' is similar to keyword 'availability' (Similarity: 1.00)\n",
            "Doc token 'notify' is similar to keyword 'notifies' (Similarity: 0.75)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'obligations' is similar to keyword 'obligations' (Similarity: 1.00)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'Criteria' is similar to keyword 'requirements' (Similarity: 0.67)\n",
            "Doc token 'notifies' is similar to keyword 'notifies' (Similarity: 1.00)\n",
            "Doc token 'accepting' is similar to keyword 'acceptance' (Similarity: 0.68)\n",
            "Doc token 'Business' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'Days' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'notifying' is similar to keyword 'notifies' (Similarity: 0.77)\n",
            "Doc token 'accepting' is similar to keyword 'acceptance' (Similarity: 0.68)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'Criteria' is similar to keyword 'requirements' (Similarity: 0.67)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'Criteria' is similar to keyword 'requirements' (Similarity: 0.67)\n",
            "Doc token 'specifications' is similar to keyword 'requirements' (Similarity: 0.65)\n",
            "Doc token 'comply' is similar to keyword 'requirements' (Similarity: 0.70)\n",
            "Doc token 'comply' is similar to keyword 'obligations' (Similarity: 0.62)\n",
            "Doc token 'requirements' is similar to keyword 'requirements' (Similarity: 1.00)\n",
            "Doc token 'obligations' is similar to keyword 'obligations' (Similarity: 1.00)\n",
            "Doc token 'agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the large English model with word vectors\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# SLA-related keywords to compare against\n",
        "keywords = [\n",
        "    \"latency\", \"throughput\", \"reliability\", \"availability\", \"jitter\",\n",
        "    \"packet\", \"loss\", \"qos\", \"acceptance\", \"notifies\",\n",
        "    \"Business Days\", \"requirements\", \"obligations\", \"agreement\"\n",
        "]\n",
        "\n",
        "# Convert keywords to spaCy tokens (handle phrases properly)\n",
        "keyword_tokens = {kw: nlp(kw)[0] if len(nlp(kw)) == 1 else nlp(kw) for kw in keywords}\n",
        "\n",
        "# Text to process\n",
        "Acceptance_of_Delivery_Clause = nlp(\"\"\"\n",
        "# The latency must be below 5 milliseconds to meet SLA.\n",
        "# Party B will have 10 Business Days to evaluate the service availability and notify Party A.\n",
        "Acceptance of Delivery.\n",
        "\"Party A\" will be deemed to have completed its delivery obligations if in \"Party B\"'s opinion, the \"Widgets\" satisfies the Acceptance Criteria, and \"Party B\" notifies \"Party A\" in writing that it is accepting the \"Widgets\".\n",
        "\n",
        "Inspection and Notice.\n",
        "\"Party B\" will have 10 Business Days to inspect and evaluate the \"Widgets\" on the delivery date before notifying \"Party A\" that it is either accepting or rejecting the \"Widgets\".\n",
        "\n",
        "Acceptance Criteria.\n",
        "The \"Acceptance Criteria\" are the specifications the \"Widgets\" must meet for \"Party A\" to comply with its requirements and obligations under this agreement, detailed in \"Attachment X\", attached to this agreement.\n",
        "\"\"\")\n",
        "\n",
        "# Compare each token in the doc Acceptance_of_Delivery_Clause to each keyword\n",
        "for token in Acceptance_of_Delivery_Clause:\n",
        "    if token.has_vector:\n",
        "        for kw, kw_token in keyword_tokens.items():\n",
        "            if isinstance(kw_token, spacy.tokens.Token):\n",
        "                # Single-word keyword\n",
        "                similarity = token.similarity(kw_token)\n",
        "            else:\n",
        "                # Multi-word keyword (e.g., \"Business Days\") ‚Äî average similarity\n",
        "                similarity = sum(token.similarity(t) for t in kw_token) / len(kw_token)\n",
        "\n",
        "            if similarity > 0.6:\n",
        "                print(f\"Doc token '{token.text}' is similar to keyword '{kw}' (Similarity: {similarity:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLyXG53M2lAR",
        "outputId": "19849bb4-38f1-44c2-ea27-211e130f6b8b"
      },
      "id": "YLyXG53M2lAR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc token 'latency' is similar to keyword 'latency' (Similarity: 1.00)\n",
            "Doc token 'latency' is similar to keyword 'throughput' (Similarity: 0.63)\n",
            "Doc token 'latency' is similar to keyword 'jitter' (Similarity: 0.65)\n",
            "Doc token 'Business' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'Days' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'availability' is similar to keyword 'availability' (Similarity: 1.00)\n",
            "Doc token 'notify' is similar to keyword 'notifies' (Similarity: 0.75)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'obligations' is similar to keyword 'obligations' (Similarity: 1.00)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'Criteria' is similar to keyword 'requirements' (Similarity: 0.67)\n",
            "Doc token 'notifies' is similar to keyword 'notifies' (Similarity: 1.00)\n",
            "Doc token 'accepting' is similar to keyword 'acceptance' (Similarity: 0.68)\n",
            "Doc token 'Business' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'Days' is similar to keyword 'Business Days' (Similarity: 0.65)\n",
            "Doc token 'notifying' is similar to keyword 'notifies' (Similarity: 0.77)\n",
            "Doc token 'accepting' is similar to keyword 'acceptance' (Similarity: 0.68)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'Criteria' is similar to keyword 'requirements' (Similarity: 0.67)\n",
            "Doc token 'Acceptance' is similar to keyword 'acceptance' (Similarity: 1.00)\n",
            "Doc token 'Criteria' is similar to keyword 'requirements' (Similarity: 0.67)\n",
            "Doc token 'specifications' is similar to keyword 'requirements' (Similarity: 0.65)\n",
            "Doc token 'comply' is similar to keyword 'requirements' (Similarity: 0.70)\n",
            "Doc token 'comply' is similar to keyword 'obligations' (Similarity: 0.62)\n",
            "Doc token 'requirements' is similar to keyword 'requirements' (Similarity: 1.00)\n",
            "Doc token 'obligations' is similar to keyword 'obligations' (Similarity: 1.00)\n",
            "Doc token 'agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Load the large English model with word vectors\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# SLA-related keywords to compare against\n",
        "keywords = [\n",
        "    \"latency\", \"throughput\", \"reliability\", \"availability\", \"jitter\",\n",
        "    \"packet\", \"loss\", \"qos\", \"acceptance\", \"notifies\", \"name\", \"License\", \"agreement\", \"SLA\", \"Licensor\", \"Company\",\n",
        "    \"Business Days\", \"requirements\", \"obligations\", \"agreement\", \"usd\", \"dollars\", \"date\"\n",
        "]\n",
        "\n",
        "# Convert keywords to spaCy tokens (handle phrases properly)\n",
        "keyword_tokens = {kw: nlp(kw)[0] if len(nlp(kw)) == 1 else nlp(kw) for kw in keywords}\n",
        "\n",
        "# Text to process\n",
        "Copyright_Clause = nlp(\"\"\"\n",
        "Copyright License Agreement\n",
        "\n",
        "This COPYRIGHT LICENSE AGREEMENT (the \"Agreement\"), dated as of 01/01/2018 (the \"Effective Date\"), is made by and between \"Me\" (\"Licensee\"), a \"NY\" \"Company\" with offices located at \"1 Broadway\", and \"Myself\" (\"Licensor\"), a \"NY\" \"Company\" with offices located at \"2 Broadway\".\n",
        "\n",
        "WHEREAS, Licensor solely and exclusively owns or controls the Work (as defined below) and wishes to grant to Licensee a license to the Work, and Licensee wishes to obtain a license to the Work for the uses and purposes described herein, each subject to the terms and conditions set forth herein.\n",
        "\n",
        "NOW, THEREFORE, in consideration of the mutual covenants, terms, and conditions set forth herein, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, the parties agree as follows:\n",
        "\n",
        "License.\n",
        "\n",
        "Grant of Rights. Subject to the terms and conditions of this Agreement, Licensor hereby grants to Licensee and its affiliates during the Term (as defined below) an exclusive, transferable right and license in the \"United States\" (the \"Territory\"), to reproduce, publicly perform, display, transmit, and distribute the Work, including translate, alter, modify, and create derivative works of the Work, through all media now known or hereinafter developed for purposes of \"stuff\". The \"Work\" is defined as \"other stuff\".\n",
        "\n",
        "Permissions. Licensor has obtained from all persons and entities who are, or whose trademark or other property is, identified, depicted, or otherwise referred to in the Work, such written and signed licenses, permissions, waivers, and consents (collectively, \"Permissions\" and each, individually, a \"Permission\"), including those relating to publicity, privacy, and any intellectual property rights, as are or reasonably may be expected to be necessary for Licensee to exercise its rights in the Work as permitted under this Agreement, without incurring any payment or other obligation to, or otherwise violating any right of, any such person or entity.\n",
        "\n",
        "Copyright Notices. Licensee shall ensure that its use of the Work is marked with the appropriate copyright notices specified by Licensor in a reasonably prominent position in the order and manner provided by Licensor. Licensee shall abide by the copyright laws and what are considered to be sound practices for copyright notice provisions in the Territory. Licensee shall not use any copyright notices that conflict with, confuse, or negate the notices Licensor provides and requires hereunder.\n",
        "\n",
        "{{#clause paymentClause}} Payment. As consideration in full for the rights granted herein, Licensee shall pay Licensor a one-time fee in the amount of \"one hundred US Dollars\" (100.0 USD) upon execution of this Agreement, payable as follows: \"bank transfer\". {{/clause}}\n",
        "\n",
        "General.\n",
        "\n",
        "Interpretation. For purposes of this Agreement, (a) the words \"include,\" \"includes,\" and \"including\" are deemed to be followed by the words \"without limitation\"; (b) the word \"or\" is not exclusive; and (c) the words \"herein,\" \"hereof,\" \"hereby,\" \"hereto,\" and \"hereunder\" refer to this Agreement as a whole. This Agreement is intended to be construed without regard to any presumption or rule requiring construction or interpretation against the party drafting an instrument or causing any instrument to be drafted.\n",
        "\n",
        "Entire Agreement. This Agreement, including and together with any related attachments, constitutes the sole and entire agreement of the parties with respect to the subject matter contained herein, and supersedes all prior and contemporaneous understandings, agreements, representations, and warranties, both written and oral, with respect to such subject matter.\n",
        "\n",
        "Severability. If any term or provision of this Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect the enforceability of any other term or provision of this Agreement, or invalidate or render unenforceable such term or provision in any other jurisdiction. [Upon a determination that any term or provision is invalid, illegal, or unenforceable, [the parties shall negotiate in good faith to/the court may] modify this Agreement to effect the original intent of the parties as closely as possible in order that the transactions contemplated hereby be consummated as originally contemplated to the greatest extent possible.]\n",
        "\n",
        "Assignment. Licensee may freely assign or otherwise transfer all or any of its rights, or delegate or otherwise transfer all or any of its obligations or performance, under this Agreement without Licensor's consent. This Agreement is binding upon and inures to the benefit of the parties hereto and their respective permitted successors and assigns.\n",
        "\"\"\")\n",
        "\n",
        "# Compare each token in the doc Copyright_Clause to each keyword\n",
        "for token in Copyright_Clause:\n",
        "    if token.has_vector:\n",
        "        for kw, kw_token in keyword_tokens.items():\n",
        "            if isinstance(kw_token, spacy.tokens.Token):\n",
        "                # Single-word keyword\n",
        "                similarity = token.similarity(kw_token)\n",
        "            else:\n",
        "                # Multi-word keyword (e.g., \"Business Days\") ‚Äî average similarity\n",
        "                similarity = sum(token.similarity(t) for t in kw_token) / len(kw_token)\n",
        "\n",
        "            if similarity > 0.6:\n",
        "                print(f\"Doc token '{token.text}' is similar to keyword '{kw}' (Similarity: {similarity:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uDl42rN6kVx",
        "outputId": "e18be3ac-17c7-4b46-f8fd-a6295b3f3ebb"
      },
      "id": "0uDl42rN6kVx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc token 'License' is similar to keyword 'License' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'LICENSE' is similar to keyword 'License' (Similarity: 1.00)\n",
            "Doc token 'AGREEMENT' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Date' is similar to keyword 'date' (Similarity: 1.00)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'Company' is similar to keyword 'Company' (Similarity: 1.00)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Company' is similar to keyword 'Company' (Similarity: 1.00)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'license' is similar to keyword 'License' (Similarity: 1.00)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'license' is similar to keyword 'License' (Similarity: 1.00)\n",
            "Doc token 'covenants' is similar to keyword 'obligations' (Similarity: 0.63)\n",
            "Doc token 'License' is similar to keyword 'License' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'license' is similar to keyword 'License' (Similarity: 1.00)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'licenses' is similar to keyword 'License' (Similarity: 0.85)\n",
            "Doc token 'necessary' is similar to keyword 'requirements' (Similarity: 0.65)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'obligation' is similar to keyword 'obligations' (Similarity: 0.83)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'ensure' is similar to keyword 'requirements' (Similarity: 0.63)\n",
            "Doc token 'appropriate' is similar to keyword 'requirements' (Similarity: 0.63)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'provisions' is similar to keyword 'obligations' (Similarity: 0.66)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'requires' is similar to keyword 'requirements' (Similarity: 0.65)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'pay' is similar to keyword 'dollars' (Similarity: 0.68)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Dollars' is similar to keyword 'dollars' (Similarity: 1.00)\n",
            "Doc token 'USD' is similar to keyword 'usd' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'requiring' is similar to keyword 'requirements' (Similarity: 0.63)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'agreements' is similar to keyword 'agreement' (Similarity: 0.85)\n",
            "Doc token 'agreements' is similar to keyword 'obligations' (Similarity: 0.70)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Licensee' is similar to keyword 'Licensor' (Similarity: 0.65)\n",
            "Doc token 'obligations' is similar to keyword 'obligations' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n",
            "Doc token 'Licensor' is similar to keyword 'Licensor' (Similarity: 1.00)\n",
            "Doc token 'Agreement' is similar to keyword 'agreement' (Similarity: 1.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in Copyright_Clause:\n",
        "    print(f\"Token: {token.text} -> Lemma: {token.lemma_}\")\n"
      ],
      "metadata": {
        "id": "sNTkJ7xb6_oQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbec957-157c-4a47-b60a-5f3caa017417"
      },
      "id": "sNTkJ7xb6_oQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: Copyright -> Lemma: Copyright\n",
            "Token: License -> Lemma: License\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: This -> Lemma: this\n",
            "Token: COPYRIGHT -> Lemma: copyright\n",
            "Token: LICENSE -> Lemma: LICENSE\n",
            "Token: AGREEMENT -> Lemma: AGREEMENT\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: dated -> Lemma: date\n",
            "Token: as -> Lemma: as\n",
            "Token: of -> Lemma: of\n",
            "Token: 01/01/2018 -> Lemma: 01/01/2018\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Effective -> Lemma: effective\n",
            "Token: Date -> Lemma: date\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: is -> Lemma: be\n",
            "Token: made -> Lemma: make\n",
            "Token: by -> Lemma: by\n",
            "Token: and -> Lemma: and\n",
            "Token: between -> Lemma: between\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Me -> Lemma: I\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: a -> Lemma: a\n",
            "Token: \" -> Lemma: \"\n",
            "Token: NY -> Lemma: NY\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Company -> Lemma: company\n",
            "Token: \" -> Lemma: \"\n",
            "Token: with -> Lemma: with\n",
            "Token: offices -> Lemma: office\n",
            "Token: located -> Lemma: locate\n",
            "Token: at -> Lemma: at\n",
            "Token: \" -> Lemma: \"\n",
            "Token: 1 -> Lemma: 1\n",
            "Token: Broadway -> Lemma: Broadway\n",
            "Token: \" -> Lemma: \"\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Myself -> Lemma: myself\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: a -> Lemma: a\n",
            "Token: \" -> Lemma: \"\n",
            "Token: NY -> Lemma: NY\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Company -> Lemma: company\n",
            "Token: \" -> Lemma: \"\n",
            "Token: with -> Lemma: with\n",
            "Token: offices -> Lemma: office\n",
            "Token: located -> Lemma: locate\n",
            "Token: at -> Lemma: at\n",
            "Token: \" -> Lemma: \"\n",
            "Token: 2 -> Lemma: 2\n",
            "Token: Broadway -> Lemma: Broadway\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: WHEREAS -> Lemma: whereas\n",
            "Token: , -> Lemma: ,\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: solely -> Lemma: solely\n",
            "Token: and -> Lemma: and\n",
            "Token: exclusively -> Lemma: exclusively\n",
            "Token: owns -> Lemma: own\n",
            "Token: or -> Lemma: or\n",
            "Token: controls -> Lemma: control\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: ( -> Lemma: (\n",
            "Token: as -> Lemma: as\n",
            "Token: defined -> Lemma: define\n",
            "Token: below -> Lemma: below\n",
            "Token: ) -> Lemma: )\n",
            "Token: and -> Lemma: and\n",
            "Token: wishes -> Lemma: wish\n",
            "Token: to -> Lemma: to\n",
            "Token: grant -> Lemma: grant\n",
            "Token: to -> Lemma: to\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: a -> Lemma: a\n",
            "Token: license -> Lemma: license\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: wishes -> Lemma: wish\n",
            "Token: to -> Lemma: to\n",
            "Token: obtain -> Lemma: obtain\n",
            "Token: a -> Lemma: a\n",
            "Token: license -> Lemma: license\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: for -> Lemma: for\n",
            "Token: the -> Lemma: the\n",
            "Token: uses -> Lemma: use\n",
            "Token: and -> Lemma: and\n",
            "Token: purposes -> Lemma: purpose\n",
            "Token: described -> Lemma: describe\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: each -> Lemma: each\n",
            "Token: subject -> Lemma: subject\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: terms -> Lemma: term\n",
            "Token: and -> Lemma: and\n",
            "Token: conditions -> Lemma: condition\n",
            "Token: set -> Lemma: set\n",
            "Token: forth -> Lemma: forth\n",
            "Token: herein -> Lemma: herein\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: NOW -> Lemma: now\n",
            "Token: , -> Lemma: ,\n",
            "Token: THEREFORE -> Lemma: therefore\n",
            "Token: , -> Lemma: ,\n",
            "Token: in -> Lemma: in\n",
            "Token: consideration -> Lemma: consideration\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: mutual -> Lemma: mutual\n",
            "Token: covenants -> Lemma: covenant\n",
            "Token: , -> Lemma: ,\n",
            "Token: terms -> Lemma: term\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: conditions -> Lemma: condition\n",
            "Token: set -> Lemma: set\n",
            "Token: forth -> Lemma: forth\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: for -> Lemma: for\n",
            "Token: other -> Lemma: other\n",
            "Token: good -> Lemma: good\n",
            "Token: and -> Lemma: and\n",
            "Token: valuable -> Lemma: valuable\n",
            "Token: consideration -> Lemma: consideration\n",
            "Token: , -> Lemma: ,\n",
            "Token: the -> Lemma: the\n",
            "Token: receipt -> Lemma: receipt\n",
            "Token: and -> Lemma: and\n",
            "Token: sufficiency -> Lemma: sufficiency\n",
            "Token: of -> Lemma: of\n",
            "Token: which -> Lemma: which\n",
            "Token: are -> Lemma: be\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: acknowledged -> Lemma: acknowledge\n",
            "Token: , -> Lemma: ,\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: agree -> Lemma: agree\n",
            "Token: as -> Lemma: as\n",
            "Token: follows -> Lemma: follow\n",
            "Token: : -> Lemma: :\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: License -> Lemma: License\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Grant -> Lemma: Grant\n",
            "Token: of -> Lemma: of\n",
            "Token: Rights -> Lemma: Rights\n",
            "Token: . -> Lemma: .\n",
            "Token: Subject -> Lemma: subject\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: terms -> Lemma: term\n",
            "Token: and -> Lemma: and\n",
            "Token: conditions -> Lemma: condition\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: grants -> Lemma: grant\n",
            "Token: to -> Lemma: to\n",
            "Token: Licensee -> Lemma: Licensee\n",
            "Token: and -> Lemma: and\n",
            "Token: its -> Lemma: its\n",
            "Token: affiliates -> Lemma: affiliate\n",
            "Token: during -> Lemma: during\n",
            "Token: the -> Lemma: the\n",
            "Token: Term -> Lemma: Term\n",
            "Token: ( -> Lemma: (\n",
            "Token: as -> Lemma: as\n",
            "Token: defined -> Lemma: define\n",
            "Token: below -> Lemma: below\n",
            "Token: ) -> Lemma: )\n",
            "Token: an -> Lemma: an\n",
            "Token: exclusive -> Lemma: exclusive\n",
            "Token: , -> Lemma: ,\n",
            "Token: transferable -> Lemma: transferable\n",
            "Token: right -> Lemma: right\n",
            "Token: and -> Lemma: and\n",
            "Token: license -> Lemma: license\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: United -> Lemma: United\n",
            "Token: States -> Lemma: States\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Territory -> Lemma: territory\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: to -> Lemma: to\n",
            "Token: reproduce -> Lemma: reproduce\n",
            "Token: , -> Lemma: ,\n",
            "Token: publicly -> Lemma: publicly\n",
            "Token: perform -> Lemma: perform\n",
            "Token: , -> Lemma: ,\n",
            "Token: display -> Lemma: display\n",
            "Token: , -> Lemma: ,\n",
            "Token: transmit -> Lemma: transmit\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: distribute -> Lemma: distribute\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: , -> Lemma: ,\n",
            "Token: including -> Lemma: include\n",
            "Token: translate -> Lemma: translate\n",
            "Token: , -> Lemma: ,\n",
            "Token: alter -> Lemma: alter\n",
            "Token: , -> Lemma: ,\n",
            "Token: modify -> Lemma: modify\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: create -> Lemma: create\n",
            "Token: derivative -> Lemma: derivative\n",
            "Token: works -> Lemma: work\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: Work\n",
            "Token: , -> Lemma: ,\n",
            "Token: through -> Lemma: through\n",
            "Token: all -> Lemma: all\n",
            "Token: media -> Lemma: medium\n",
            "Token: now -> Lemma: now\n",
            "Token: known -> Lemma: know\n",
            "Token: or -> Lemma: or\n",
            "Token: hereinafter -> Lemma: hereinafter\n",
            "Token: developed -> Lemma: develop\n",
            "Token: for -> Lemma: for\n",
            "Token: purposes -> Lemma: purpose\n",
            "Token: of -> Lemma: of\n",
            "Token: \" -> Lemma: \"\n",
            "Token: stuff -> Lemma: stuff\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: The -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Work -> Lemma: work\n",
            "Token: \" -> Lemma: \"\n",
            "Token: is -> Lemma: be\n",
            "Token: defined -> Lemma: define\n",
            "Token: as -> Lemma: as\n",
            "Token: \" -> Lemma: \"\n",
            "Token: other -> Lemma: other\n",
            "Token: stuff -> Lemma: stuff\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Permissions -> Lemma: Permissions\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: has -> Lemma: have\n",
            "Token: obtained -> Lemma: obtain\n",
            "Token: from -> Lemma: from\n",
            "Token: all -> Lemma: all\n",
            "Token: persons -> Lemma: person\n",
            "Token: and -> Lemma: and\n",
            "Token: entities -> Lemma: entity\n",
            "Token: who -> Lemma: who\n",
            "Token: are -> Lemma: be\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: whose -> Lemma: whose\n",
            "Token: trademark -> Lemma: trademark\n",
            "Token: or -> Lemma: or\n",
            "Token: other -> Lemma: other\n",
            "Token: property -> Lemma: property\n",
            "Token: is -> Lemma: be\n",
            "Token: , -> Lemma: ,\n",
            "Token: identified -> Lemma: identify\n",
            "Token: , -> Lemma: ,\n",
            "Token: depicted -> Lemma: depict\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: referred -> Lemma: refer\n",
            "Token: to -> Lemma: to\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: Work\n",
            "Token: , -> Lemma: ,\n",
            "Token: such -> Lemma: such\n",
            "Token: written -> Lemma: write\n",
            "Token: and -> Lemma: and\n",
            "Token: signed -> Lemma: sign\n",
            "Token: licenses -> Lemma: license\n",
            "Token: , -> Lemma: ,\n",
            "Token: permissions -> Lemma: permission\n",
            "Token: , -> Lemma: ,\n",
            "Token: waivers -> Lemma: waiver\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: consents -> Lemma: consent\n",
            "Token: ( -> Lemma: (\n",
            "Token: collectively -> Lemma: collectively\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Permissions -> Lemma: Permissions\n",
            "Token: \" -> Lemma: \"\n",
            "Token: and -> Lemma: and\n",
            "Token: each -> Lemma: each\n",
            "Token: , -> Lemma: ,\n",
            "Token: individually -> Lemma: individually\n",
            "Token: , -> Lemma: ,\n",
            "Token: a -> Lemma: a\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Permission -> Lemma: Permission\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: including -> Lemma: include\n",
            "Token: those -> Lemma: those\n",
            "Token: relating -> Lemma: relate\n",
            "Token: to -> Lemma: to\n",
            "Token: publicity -> Lemma: publicity\n",
            "Token: , -> Lemma: ,\n",
            "Token: privacy -> Lemma: privacy\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: any -> Lemma: any\n",
            "Token: intellectual -> Lemma: intellectual\n",
            "Token: property -> Lemma: property\n",
            "Token: rights -> Lemma: right\n",
            "Token: , -> Lemma: ,\n",
            "Token: as -> Lemma: as\n",
            "Token: are -> Lemma: be\n",
            "Token: or -> Lemma: or\n",
            "Token: reasonably -> Lemma: reasonably\n",
            "Token: may -> Lemma: may\n",
            "Token: be -> Lemma: be\n",
            "Token: expected -> Lemma: expect\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: necessary -> Lemma: necessary\n",
            "Token: for -> Lemma: for\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: to -> Lemma: to\n",
            "Token: exercise -> Lemma: exercise\n",
            "Token: its -> Lemma: its\n",
            "Token: rights -> Lemma: right\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: as -> Lemma: as\n",
            "Token: permitted -> Lemma: permit\n",
            "Token: under -> Lemma: under\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: without -> Lemma: without\n",
            "Token: incurring -> Lemma: incur\n",
            "Token: any -> Lemma: any\n",
            "Token: payment -> Lemma: payment\n",
            "Token: or -> Lemma: or\n",
            "Token: other -> Lemma: other\n",
            "Token: obligation -> Lemma: obligation\n",
            "Token: to -> Lemma: to\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: violating -> Lemma: violate\n",
            "Token: any -> Lemma: any\n",
            "Token: right -> Lemma: right\n",
            "Token: of -> Lemma: of\n",
            "Token: , -> Lemma: ,\n",
            "Token: any -> Lemma: any\n",
            "Token: such -> Lemma: such\n",
            "Token: person -> Lemma: person\n",
            "Token: or -> Lemma: or\n",
            "Token: entity -> Lemma: entity\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Copyright -> Lemma: Copyright\n",
            "Token: Notices -> Lemma: Notices\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: shall -> Lemma: shall\n",
            "Token: ensure -> Lemma: ensure\n",
            "Token: that -> Lemma: that\n",
            "Token: its -> Lemma: its\n",
            "Token: use -> Lemma: use\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: is -> Lemma: be\n",
            "Token: marked -> Lemma: mark\n",
            "Token: with -> Lemma: with\n",
            "Token: the -> Lemma: the\n",
            "Token: appropriate -> Lemma: appropriate\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: notices -> Lemma: notice\n",
            "Token: specified -> Lemma: specify\n",
            "Token: by -> Lemma: by\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: in -> Lemma: in\n",
            "Token: a -> Lemma: a\n",
            "Token: reasonably -> Lemma: reasonably\n",
            "Token: prominent -> Lemma: prominent\n",
            "Token: position -> Lemma: position\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: order -> Lemma: order\n",
            "Token: and -> Lemma: and\n",
            "Token: manner -> Lemma: manner\n",
            "Token: provided -> Lemma: provide\n",
            "Token: by -> Lemma: by\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: Licensee\n",
            "Token: shall -> Lemma: shall\n",
            "Token: abide -> Lemma: abide\n",
            "Token: by -> Lemma: by\n",
            "Token: the -> Lemma: the\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: laws -> Lemma: law\n",
            "Token: and -> Lemma: and\n",
            "Token: what -> Lemma: what\n",
            "Token: are -> Lemma: be\n",
            "Token: considered -> Lemma: consider\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: sound -> Lemma: sound\n",
            "Token: practices -> Lemma: practice\n",
            "Token: for -> Lemma: for\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: notice -> Lemma: notice\n",
            "Token: provisions -> Lemma: provision\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: Territory -> Lemma: Territory\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: shall -> Lemma: shall\n",
            "Token: not -> Lemma: not\n",
            "Token: use -> Lemma: use\n",
            "Token: any -> Lemma: any\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: notices -> Lemma: notice\n",
            "Token: that -> Lemma: that\n",
            "Token: conflict -> Lemma: conflict\n",
            "Token: with -> Lemma: with\n",
            "Token: , -> Lemma: ,\n",
            "Token: confuse -> Lemma: confuse\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: negate -> Lemma: negate\n",
            "Token: the -> Lemma: the\n",
            "Token: notices -> Lemma: notice\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: provides -> Lemma: provide\n",
            "Token: and -> Lemma: and\n",
            "Token: requires -> Lemma: require\n",
            "Token: hereunder -> Lemma: hereunder\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: { -> Lemma: {\n",
            "Token: { -> Lemma: {\n",
            "Token: # -> Lemma: #\n",
            "Token: clause -> Lemma: clause\n",
            "Token: paymentClause -> Lemma: paymentclause\n",
            "Token: } -> Lemma: }\n",
            "Token: } -> Lemma: }\n",
            "Token: Payment -> Lemma: Payment\n",
            "Token: . -> Lemma: .\n",
            "Token: As -> Lemma: as\n",
            "Token: consideration -> Lemma: consideration\n",
            "Token: in -> Lemma: in\n",
            "Token: full -> Lemma: full\n",
            "Token: for -> Lemma: for\n",
            "Token: the -> Lemma: the\n",
            "Token: rights -> Lemma: right\n",
            "Token: granted -> Lemma: grant\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: Licensee -> Lemma: Licensee\n",
            "Token: shall -> Lemma: shall\n",
            "Token: pay -> Lemma: pay\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: a -> Lemma: a\n",
            "Token: one -> Lemma: one\n",
            "Token: - -> Lemma: -\n",
            "Token: time -> Lemma: time\n",
            "Token: fee -> Lemma: fee\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: amount -> Lemma: amount\n",
            "Token: of -> Lemma: of\n",
            "Token: \" -> Lemma: \"\n",
            "Token: one -> Lemma: one\n",
            "Token: hundred -> Lemma: hundred\n",
            "Token: US -> Lemma: US\n",
            "Token: Dollars -> Lemma: Dollars\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: 100.0 -> Lemma: 100.0\n",
            "Token: USD -> Lemma: USD\n",
            "Token: ) -> Lemma: )\n",
            "Token: upon -> Lemma: upon\n",
            "Token: execution -> Lemma: execution\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: payable -> Lemma: payable\n",
            "Token: as -> Lemma: as\n",
            "Token: follows -> Lemma: follow\n",
            "Token: : -> Lemma: :\n",
            "Token: \" -> Lemma: \"\n",
            "Token: bank -> Lemma: bank\n",
            "Token: transfer -> Lemma: transfer\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: { -> Lemma: {\n",
            "Token: { -> Lemma: {\n",
            "Token: /clause -> Lemma: /clause\n",
            "Token: } -> Lemma: }\n",
            "Token: } -> Lemma: }\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: General -> Lemma: General\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Interpretation -> Lemma: Interpretation\n",
            "Token: . -> Lemma: .\n",
            "Token: For -> Lemma: for\n",
            "Token: purposes -> Lemma: purpose\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: ( -> Lemma: (\n",
            "Token: a -> Lemma: a\n",
            "Token: ) -> Lemma: )\n",
            "Token: the -> Lemma: the\n",
            "Token: words -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: include -> Lemma: include\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: includes -> Lemma: include\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: and -> Lemma: and\n",
            "Token: \" -> Lemma: \"\n",
            "Token: including -> Lemma: include\n",
            "Token: \" -> Lemma: \"\n",
            "Token: are -> Lemma: be\n",
            "Token: deemed -> Lemma: deem\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: followed -> Lemma: follow\n",
            "Token: by -> Lemma: by\n",
            "Token: the -> Lemma: the\n",
            "Token: words -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: without -> Lemma: without\n",
            "Token: limitation -> Lemma: limitation\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ; -> Lemma: ;\n",
            "Token: ( -> Lemma: (\n",
            "Token: b -> Lemma: b\n",
            "Token: ) -> Lemma: )\n",
            "Token: the -> Lemma: the\n",
            "Token: word -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: or -> Lemma: or\n",
            "Token: \" -> Lemma: \"\n",
            "Token: is -> Lemma: be\n",
            "Token: not -> Lemma: not\n",
            "Token: exclusive -> Lemma: exclusive\n",
            "Token: ; -> Lemma: ;\n",
            "Token: and -> Lemma: and\n",
            "Token: ( -> Lemma: (\n",
            "Token: c -> Lemma: c\n",
            "Token: ) -> Lemma: )\n",
            "Token: the -> Lemma: the\n",
            "Token: words -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereof -> Lemma: hereof\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereto -> Lemma: hereto\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: and -> Lemma: and\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereunder -> Lemma: hereunder\n",
            "Token: \" -> Lemma: \"\n",
            "Token: refer -> Lemma: refer\n",
            "Token: to -> Lemma: to\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: as -> Lemma: as\n",
            "Token: a -> Lemma: a\n",
            "Token: whole -> Lemma: whole\n",
            "Token: . -> Lemma: .\n",
            "Token: This -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: is -> Lemma: be\n",
            "Token: intended -> Lemma: intend\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: construed -> Lemma: construe\n",
            "Token: without -> Lemma: without\n",
            "Token: regard -> Lemma: regard\n",
            "Token: to -> Lemma: to\n",
            "Token: any -> Lemma: any\n",
            "Token: presumption -> Lemma: presumption\n",
            "Token: or -> Lemma: or\n",
            "Token: rule -> Lemma: rule\n",
            "Token: requiring -> Lemma: require\n",
            "Token: construction -> Lemma: construction\n",
            "Token: or -> Lemma: or\n",
            "Token: interpretation -> Lemma: interpretation\n",
            "Token: against -> Lemma: against\n",
            "Token: the -> Lemma: the\n",
            "Token: party -> Lemma: party\n",
            "Token: drafting -> Lemma: draft\n",
            "Token: an -> Lemma: an\n",
            "Token: instrument -> Lemma: instrument\n",
            "Token: or -> Lemma: or\n",
            "Token: causing -> Lemma: cause\n",
            "Token: any -> Lemma: any\n",
            "Token: instrument -> Lemma: instrument\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: drafted -> Lemma: draft\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Entire -> Lemma: Entire\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: . -> Lemma: .\n",
            "Token: This -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: including -> Lemma: include\n",
            "Token: and -> Lemma: and\n",
            "Token: together -> Lemma: together\n",
            "Token: with -> Lemma: with\n",
            "Token: any -> Lemma: any\n",
            "Token: related -> Lemma: related\n",
            "Token: attachments -> Lemma: attachment\n",
            "Token: , -> Lemma: ,\n",
            "Token: constitutes -> Lemma: constitute\n",
            "Token: the -> Lemma: the\n",
            "Token: sole -> Lemma: sole\n",
            "Token: and -> Lemma: and\n",
            "Token: entire -> Lemma: entire\n",
            "Token: agreement -> Lemma: agreement\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: with -> Lemma: with\n",
            "Token: respect -> Lemma: respect\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: subject -> Lemma: subject\n",
            "Token: matter -> Lemma: matter\n",
            "Token: contained -> Lemma: contain\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: supersedes -> Lemma: supersede\n",
            "Token: all -> Lemma: all\n",
            "Token: prior -> Lemma: prior\n",
            "Token: and -> Lemma: and\n",
            "Token: contemporaneous -> Lemma: contemporaneous\n",
            "Token: understandings -> Lemma: understanding\n",
            "Token: , -> Lemma: ,\n",
            "Token: agreements -> Lemma: agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: representations -> Lemma: representation\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: warranties -> Lemma: warranty\n",
            "Token: , -> Lemma: ,\n",
            "Token: both -> Lemma: both\n",
            "Token: written -> Lemma: write\n",
            "Token: and -> Lemma: and\n",
            "Token: oral -> Lemma: oral\n",
            "Token: , -> Lemma: ,\n",
            "Token: with -> Lemma: with\n",
            "Token: respect -> Lemma: respect\n",
            "Token: to -> Lemma: to\n",
            "Token: such -> Lemma: such\n",
            "Token: subject -> Lemma: subject\n",
            "Token: matter -> Lemma: matter\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Severability -> Lemma: Severability\n",
            "Token: . -> Lemma: .\n",
            "Token: If -> Lemma: if\n",
            "Token: any -> Lemma: any\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: is -> Lemma: be\n",
            "Token: invalid -> Lemma: invalid\n",
            "Token: , -> Lemma: ,\n",
            "Token: illegal -> Lemma: illegal\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: unenforceable -> Lemma: unenforceable\n",
            "Token: in -> Lemma: in\n",
            "Token: any -> Lemma: any\n",
            "Token: jurisdiction -> Lemma: jurisdiction\n",
            "Token: , -> Lemma: ,\n",
            "Token: such -> Lemma: such\n",
            "Token: invalidity -> Lemma: invalidity\n",
            "Token: , -> Lemma: ,\n",
            "Token: illegality -> Lemma: illegality\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: unenforceability -> Lemma: unenforceability\n",
            "Token: will -> Lemma: will\n",
            "Token: not -> Lemma: not\n",
            "Token: affect -> Lemma: affect\n",
            "Token: the -> Lemma: the\n",
            "Token: enforceability -> Lemma: enforceability\n",
            "Token: of -> Lemma: of\n",
            "Token: any -> Lemma: any\n",
            "Token: other -> Lemma: other\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: invalidate -> Lemma: invalidate\n",
            "Token: or -> Lemma: or\n",
            "Token: render -> Lemma: render\n",
            "Token: unenforceable -> Lemma: unenforceable\n",
            "Token: such -> Lemma: such\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: in -> Lemma: in\n",
            "Token: any -> Lemma: any\n",
            "Token: other -> Lemma: other\n",
            "Token: jurisdiction -> Lemma: jurisdiction\n",
            "Token: . -> Lemma: .\n",
            "Token: [ -> Lemma: [\n",
            "Token: Upon -> Lemma: upon\n",
            "Token: a -> Lemma: a\n",
            "Token: determination -> Lemma: determination\n",
            "Token: that -> Lemma: that\n",
            "Token: any -> Lemma: any\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: is -> Lemma: be\n",
            "Token: invalid -> Lemma: invalid\n",
            "Token: , -> Lemma: ,\n",
            "Token: illegal -> Lemma: illegal\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: unenforceable -> Lemma: unenforceable\n",
            "Token: , -> Lemma: ,\n",
            "Token: [ -> Lemma: [\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: shall -> Lemma: shall\n",
            "Token: negotiate -> Lemma: negotiate\n",
            "Token: in -> Lemma: in\n",
            "Token: good -> Lemma: good\n",
            "Token: faith -> Lemma: faith\n",
            "Token: to -> Lemma: to\n",
            "Token: / -> Lemma: /\n",
            "Token: the -> Lemma: the\n",
            "Token: court -> Lemma: court\n",
            "Token: may -> Lemma: may\n",
            "Token: ] -> Lemma: ]\n",
            "Token: modify -> Lemma: modify\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: to -> Lemma: to\n",
            "Token: effect -> Lemma: effect\n",
            "Token: the -> Lemma: the\n",
            "Token: original -> Lemma: original\n",
            "Token: intent -> Lemma: intent\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: as -> Lemma: as\n",
            "Token: closely -> Lemma: closely\n",
            "Token: as -> Lemma: as\n",
            "Token: possible -> Lemma: possible\n",
            "Token: in -> Lemma: in\n",
            "Token: order -> Lemma: order\n",
            "Token: that -> Lemma: that\n",
            "Token: the -> Lemma: the\n",
            "Token: transactions -> Lemma: transaction\n",
            "Token: contemplated -> Lemma: contemplate\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: be -> Lemma: be\n",
            "Token: consummated -> Lemma: consummate\n",
            "Token: as -> Lemma: as\n",
            "Token: originally -> Lemma: originally\n",
            "Token: contemplated -> Lemma: contemplate\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: greatest -> Lemma: great\n",
            "Token: extent -> Lemma: extent\n",
            "Token: possible -> Lemma: possible\n",
            "Token: . -> Lemma: .\n",
            "Token: ] -> Lemma: ]\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Assignment -> Lemma: Assignment\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: licensee\n",
            "Token: may -> Lemma: may\n",
            "Token: freely -> Lemma: freely\n",
            "Token: assign -> Lemma: assign\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: transfer -> Lemma: transfer\n",
            "Token: all -> Lemma: all\n",
            "Token: or -> Lemma: or\n",
            "Token: any -> Lemma: any\n",
            "Token: of -> Lemma: of\n",
            "Token: its -> Lemma: its\n",
            "Token: rights -> Lemma: right\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: delegate -> Lemma: delegate\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: transfer -> Lemma: transfer\n",
            "Token: all -> Lemma: all\n",
            "Token: or -> Lemma: or\n",
            "Token: any -> Lemma: any\n",
            "Token: of -> Lemma: of\n",
            "Token: its -> Lemma: its\n",
            "Token: obligations -> Lemma: obligation\n",
            "Token: or -> Lemma: or\n",
            "Token: performance -> Lemma: performance\n",
            "Token: , -> Lemma: ,\n",
            "Token: under -> Lemma: under\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: without -> Lemma: without\n",
            "Token: Licensor -> Lemma: Licensor\n",
            "Token: 's -> Lemma: 's\n",
            "Token: consent -> Lemma: consent\n",
            "Token: . -> Lemma: .\n",
            "Token: This -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: is -> Lemma: be\n",
            "Token: binding -> Lemma: bind\n",
            "Token: upon -> Lemma: upon\n",
            "Token: and -> Lemma: and\n",
            "Token: inures -> Lemma: inure\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: benefit -> Lemma: benefit\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: hereto -> Lemma: hereto\n",
            "Token: and -> Lemma: and\n",
            "Token: their -> Lemma: their\n",
            "Token: respective -> Lemma: respective\n",
            "Token: permitted -> Lemma: permit\n",
            "Token: successors -> Lemma: successor\n",
            "Token: and -> Lemma: and\n",
            "Token: assigns -> Lemma: assign\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if token.text.lower() in [\"licensor\", \"licensee\"]:\n",
        "    print(f\"Custom lemma for '{token.text}' ‚Üí 'license'\")\n"
      ],
      "metadata": {
        "id": "QkJD-EG2JmQl"
      },
      "id": "QkJD-EG2JmQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the large English model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Define legal text\n",
        "legal_text = \"\"\"\n",
        "Copyright License Agreement\n",
        "\n",
        "This COPYRIGHT LICENSE AGREEMENT (the \"Agreement\"), dated as of 01/01/2018 (the \"Effective Date\"), is made by and between \"Me\" (\"Licensee\"), a \"NY\" \"Company\" with offices located at \"1 Broadway\", and \"Myself\" (\"Licensor\"), a \"NY\" \"Company\" with offices located at \"2 Broadway\".\n",
        "\n",
        "WHEREAS, Licensor solely and exclusively owns or controls the Work (as defined below) and wishes to grant to Licensee a license to the Work, and Licensee wishes to obtain a license to the Work for the uses and purposes described herein, each subject to the terms and conditions set forth herein.\n",
        "\n",
        "NOW, THEREFORE, in consideration of the mutual covenants, terms, and conditions set forth herein, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, the parties agree as follows:\n",
        "\n",
        "License.\n",
        "\n",
        "Grant of Rights. Subject to the terms and conditions of this Agreement, Licensor hereby grants to Licensee and its affiliates during the Term (as defined below) an exclusive, transferable right and license in the \"United States\" (the \"Territory\"), to reproduce, publicly perform, display, transmit, and distribute the Work, including translate, alter, modify, and create derivative works of the Work, through all media now known or hereinafter developed for purposes of \"stuff\". The \"Work\" is defined as \"other stuff\".\n",
        "\n",
        "Permissions. Licensor has obtained from all persons and entities who are, or whose trademark or other property is, identified, depicted, or otherwise referred to in the Work, such written and signed licenses, permissions, waivers, and consents (collectively, \"Permissions\" and each, individually, a \"Permission\"), including those relating to publicity, privacy, and any intellectual property rights, as are or reasonably may be expected to be necessary for Licensee to exercise its rights in the Work as permitted under this Agreement, without incurring any payment or other obligation to, or otherwise violating any right of, any such person or entity.\n",
        "\n",
        "Copyright Notices. Licensee shall ensure that its use of the Work is marked with the appropriate copyright notices specified by Licensor in a reasonably prominent position in the order and manner provided by Licensor. Licensee shall abide by the copyright laws and what are considered to be sound practices for copyright notice provisions in the Territory. Licensee shall not use any copyright notices that conflict with, confuse, or negate the notices Licensor provides and requires hereunder.\n",
        "\n",
        "Payment. As consideration in full for the rights granted herein, Licensee shall pay Licensor a one-time fee in the amount of \"one hundred US Dollars\" (100.0 USD) upon execution of this Agreement, payable as follows: \"bank transfer\".\n",
        "\n",
        "General.\n",
        "\n",
        "Interpretation. For purposes of this Agreement, (a) the words \"include,\" \"includes,\" and \"including\" are deemed to be followed by the words \"without limitation\"; (b) the word \"or\" is not exclusive; and (c) the words \"herein,\" \"hereof,\" \"hereby,\" \"hereto,\" and \"hereunder\" refer to this Agreement as a whole. This Agreement is intended to be construed without regard to any presumption or rule requiring construction or interpretation against the party drafting an instrument or causing any instrument to be drafted.\n",
        "\n",
        "Entire Agreement. This Agreement, including and together with any related attachments, constitutes the sole and entire agreement of the parties with respect to the subject matter contained herein, and supersedes all prior and contemporaneous understandings, agreements, representations, and warranties, both written and oral, with respect to such subject matter.\n",
        "\n",
        "Severability. If any term or provision of this Agreement is invalid, illegal, or unenforceable in any jurisdiction, such invalidity, illegality, or unenforceability will not affect the enforceability of any other term or provision of this Agreement, or invalidate or render unenforceable such term or provision in any other jurisdiction. [Upon a determination that any term or provision is invalid, illegal, or unenforceable, [the parties shall negotiate in good faith to/the court may] modify this Agreement to effect the original intent of the parties as closely as possible in order that the transactions contemplated hereby be consummated as originally contemplated to the greatest extent possible.]\n",
        "\n",
        "Assignment. Licensee may freely assign or otherwise transfer all or any of its rights, or delegate or otherwise transfer all or any of its obligations or performance, under this Agreement without Licensor's consent. This Agreement is binding upon and inures to the benefit of the parties hereto and their respective permitted successors and assigns.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(legal_text)\n",
        "\n",
        "# Optional: define custom overrides for certain proper nouns\n",
        "custom_lemma_map = {\n",
        "    \"Licensor\": \"license\",\n",
        "    \"Licensee\": \"license\"\n",
        "}\n",
        "\n",
        "# Print tokens and their lemmas\n",
        "for token in doc:\n",
        "    if token.text in custom_lemma_map:\n",
        "        lemma = custom_lemma_map[token.text]\n",
        "    else:\n",
        "        lemma = token.lemma_\n",
        "    print(f\"Token: {token.text} -> Lemma: {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4msCNN6xJnWR",
        "outputId": "97190c53-d4c3-4da1-d6db-eb3c5187f3ad"
      },
      "id": "4msCNN6xJnWR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: \n",
            " -> Lemma: \n",
            "\n",
            "Token: Copyright -> Lemma: Copyright\n",
            "Token: License -> Lemma: License\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: This -> Lemma: this\n",
            "Token: COPYRIGHT -> Lemma: copyright\n",
            "Token: LICENSE -> Lemma: LICENSE\n",
            "Token: AGREEMENT -> Lemma: AGREEMENT\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: dated -> Lemma: date\n",
            "Token: as -> Lemma: as\n",
            "Token: of -> Lemma: of\n",
            "Token: 01/01/2018 -> Lemma: 01/01/2018\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Effective -> Lemma: effective\n",
            "Token: Date -> Lemma: date\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: is -> Lemma: be\n",
            "Token: made -> Lemma: make\n",
            "Token: by -> Lemma: by\n",
            "Token: and -> Lemma: and\n",
            "Token: between -> Lemma: between\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Me -> Lemma: I\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: a -> Lemma: a\n",
            "Token: \" -> Lemma: \"\n",
            "Token: NY -> Lemma: NY\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Company -> Lemma: company\n",
            "Token: \" -> Lemma: \"\n",
            "Token: with -> Lemma: with\n",
            "Token: offices -> Lemma: office\n",
            "Token: located -> Lemma: locate\n",
            "Token: at -> Lemma: at\n",
            "Token: \" -> Lemma: \"\n",
            "Token: 1 -> Lemma: 1\n",
            "Token: Broadway -> Lemma: Broadway\n",
            "Token: \" -> Lemma: \"\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Myself -> Lemma: myself\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: a -> Lemma: a\n",
            "Token: \" -> Lemma: \"\n",
            "Token: NY -> Lemma: NY\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Company -> Lemma: company\n",
            "Token: \" -> Lemma: \"\n",
            "Token: with -> Lemma: with\n",
            "Token: offices -> Lemma: office\n",
            "Token: located -> Lemma: locate\n",
            "Token: at -> Lemma: at\n",
            "Token: \" -> Lemma: \"\n",
            "Token: 2 -> Lemma: 2\n",
            "Token: Broadway -> Lemma: Broadway\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: WHEREAS -> Lemma: whereas\n",
            "Token: , -> Lemma: ,\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: solely -> Lemma: solely\n",
            "Token: and -> Lemma: and\n",
            "Token: exclusively -> Lemma: exclusively\n",
            "Token: owns -> Lemma: own\n",
            "Token: or -> Lemma: or\n",
            "Token: controls -> Lemma: control\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: ( -> Lemma: (\n",
            "Token: as -> Lemma: as\n",
            "Token: defined -> Lemma: define\n",
            "Token: below -> Lemma: below\n",
            "Token: ) -> Lemma: )\n",
            "Token: and -> Lemma: and\n",
            "Token: wishes -> Lemma: wish\n",
            "Token: to -> Lemma: to\n",
            "Token: grant -> Lemma: grant\n",
            "Token: to -> Lemma: to\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: a -> Lemma: a\n",
            "Token: license -> Lemma: license\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: wishes -> Lemma: wish\n",
            "Token: to -> Lemma: to\n",
            "Token: obtain -> Lemma: obtain\n",
            "Token: a -> Lemma: a\n",
            "Token: license -> Lemma: license\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: for -> Lemma: for\n",
            "Token: the -> Lemma: the\n",
            "Token: uses -> Lemma: use\n",
            "Token: and -> Lemma: and\n",
            "Token: purposes -> Lemma: purpose\n",
            "Token: described -> Lemma: describe\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: each -> Lemma: each\n",
            "Token: subject -> Lemma: subject\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: terms -> Lemma: term\n",
            "Token: and -> Lemma: and\n",
            "Token: conditions -> Lemma: condition\n",
            "Token: set -> Lemma: set\n",
            "Token: forth -> Lemma: forth\n",
            "Token: herein -> Lemma: herein\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: NOW -> Lemma: now\n",
            "Token: , -> Lemma: ,\n",
            "Token: THEREFORE -> Lemma: therefore\n",
            "Token: , -> Lemma: ,\n",
            "Token: in -> Lemma: in\n",
            "Token: consideration -> Lemma: consideration\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: mutual -> Lemma: mutual\n",
            "Token: covenants -> Lemma: covenant\n",
            "Token: , -> Lemma: ,\n",
            "Token: terms -> Lemma: term\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: conditions -> Lemma: condition\n",
            "Token: set -> Lemma: set\n",
            "Token: forth -> Lemma: forth\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: for -> Lemma: for\n",
            "Token: other -> Lemma: other\n",
            "Token: good -> Lemma: good\n",
            "Token: and -> Lemma: and\n",
            "Token: valuable -> Lemma: valuable\n",
            "Token: consideration -> Lemma: consideration\n",
            "Token: , -> Lemma: ,\n",
            "Token: the -> Lemma: the\n",
            "Token: receipt -> Lemma: receipt\n",
            "Token: and -> Lemma: and\n",
            "Token: sufficiency -> Lemma: sufficiency\n",
            "Token: of -> Lemma: of\n",
            "Token: which -> Lemma: which\n",
            "Token: are -> Lemma: be\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: acknowledged -> Lemma: acknowledge\n",
            "Token: , -> Lemma: ,\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: agree -> Lemma: agree\n",
            "Token: as -> Lemma: as\n",
            "Token: follows -> Lemma: follow\n",
            "Token: : -> Lemma: :\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: License -> Lemma: License\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Grant -> Lemma: Grant\n",
            "Token: of -> Lemma: of\n",
            "Token: Rights -> Lemma: Rights\n",
            "Token: . -> Lemma: .\n",
            "Token: Subject -> Lemma: subject\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: terms -> Lemma: term\n",
            "Token: and -> Lemma: and\n",
            "Token: conditions -> Lemma: condition\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: grants -> Lemma: grant\n",
            "Token: to -> Lemma: to\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: and -> Lemma: and\n",
            "Token: its -> Lemma: its\n",
            "Token: affiliates -> Lemma: affiliate\n",
            "Token: during -> Lemma: during\n",
            "Token: the -> Lemma: the\n",
            "Token: Term -> Lemma: Term\n",
            "Token: ( -> Lemma: (\n",
            "Token: as -> Lemma: as\n",
            "Token: defined -> Lemma: define\n",
            "Token: below -> Lemma: below\n",
            "Token: ) -> Lemma: )\n",
            "Token: an -> Lemma: an\n",
            "Token: exclusive -> Lemma: exclusive\n",
            "Token: , -> Lemma: ,\n",
            "Token: transferable -> Lemma: transferable\n",
            "Token: right -> Lemma: right\n",
            "Token: and -> Lemma: and\n",
            "Token: license -> Lemma: license\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: United -> Lemma: United\n",
            "Token: States -> Lemma: States\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: the -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Territory -> Lemma: territory\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: to -> Lemma: to\n",
            "Token: reproduce -> Lemma: reproduce\n",
            "Token: , -> Lemma: ,\n",
            "Token: publicly -> Lemma: publicly\n",
            "Token: perform -> Lemma: perform\n",
            "Token: , -> Lemma: ,\n",
            "Token: display -> Lemma: display\n",
            "Token: , -> Lemma: ,\n",
            "Token: transmit -> Lemma: transmit\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: distribute -> Lemma: distribute\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: , -> Lemma: ,\n",
            "Token: including -> Lemma: include\n",
            "Token: translate -> Lemma: translate\n",
            "Token: , -> Lemma: ,\n",
            "Token: alter -> Lemma: alter\n",
            "Token: , -> Lemma: ,\n",
            "Token: modify -> Lemma: modify\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: create -> Lemma: create\n",
            "Token: derivative -> Lemma: derivative\n",
            "Token: works -> Lemma: work\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: Work\n",
            "Token: , -> Lemma: ,\n",
            "Token: through -> Lemma: through\n",
            "Token: all -> Lemma: all\n",
            "Token: media -> Lemma: medium\n",
            "Token: now -> Lemma: now\n",
            "Token: known -> Lemma: know\n",
            "Token: or -> Lemma: or\n",
            "Token: hereinafter -> Lemma: hereinafter\n",
            "Token: developed -> Lemma: develop\n",
            "Token: for -> Lemma: for\n",
            "Token: purposes -> Lemma: purpose\n",
            "Token: of -> Lemma: of\n",
            "Token: \" -> Lemma: \"\n",
            "Token: stuff -> Lemma: stuff\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: The -> Lemma: the\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Work -> Lemma: work\n",
            "Token: \" -> Lemma: \"\n",
            "Token: is -> Lemma: be\n",
            "Token: defined -> Lemma: define\n",
            "Token: as -> Lemma: as\n",
            "Token: \" -> Lemma: \"\n",
            "Token: other -> Lemma: other\n",
            "Token: stuff -> Lemma: stuff\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Permissions -> Lemma: Permissions\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: has -> Lemma: have\n",
            "Token: obtained -> Lemma: obtain\n",
            "Token: from -> Lemma: from\n",
            "Token: all -> Lemma: all\n",
            "Token: persons -> Lemma: person\n",
            "Token: and -> Lemma: and\n",
            "Token: entities -> Lemma: entity\n",
            "Token: who -> Lemma: who\n",
            "Token: are -> Lemma: be\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: whose -> Lemma: whose\n",
            "Token: trademark -> Lemma: trademark\n",
            "Token: or -> Lemma: or\n",
            "Token: other -> Lemma: other\n",
            "Token: property -> Lemma: property\n",
            "Token: is -> Lemma: be\n",
            "Token: , -> Lemma: ,\n",
            "Token: identified -> Lemma: identify\n",
            "Token: , -> Lemma: ,\n",
            "Token: depicted -> Lemma: depict\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: referred -> Lemma: refer\n",
            "Token: to -> Lemma: to\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: Work\n",
            "Token: , -> Lemma: ,\n",
            "Token: such -> Lemma: such\n",
            "Token: written -> Lemma: write\n",
            "Token: and -> Lemma: and\n",
            "Token: signed -> Lemma: sign\n",
            "Token: licenses -> Lemma: license\n",
            "Token: , -> Lemma: ,\n",
            "Token: permissions -> Lemma: permission\n",
            "Token: , -> Lemma: ,\n",
            "Token: waivers -> Lemma: waiver\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: consents -> Lemma: consent\n",
            "Token: ( -> Lemma: (\n",
            "Token: collectively -> Lemma: collectively\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Permissions -> Lemma: Permissions\n",
            "Token: \" -> Lemma: \"\n",
            "Token: and -> Lemma: and\n",
            "Token: each -> Lemma: each\n",
            "Token: , -> Lemma: ,\n",
            "Token: individually -> Lemma: individually\n",
            "Token: , -> Lemma: ,\n",
            "Token: a -> Lemma: a\n",
            "Token: \" -> Lemma: \"\n",
            "Token: Permission -> Lemma: Permission\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ) -> Lemma: )\n",
            "Token: , -> Lemma: ,\n",
            "Token: including -> Lemma: include\n",
            "Token: those -> Lemma: those\n",
            "Token: relating -> Lemma: relate\n",
            "Token: to -> Lemma: to\n",
            "Token: publicity -> Lemma: publicity\n",
            "Token: , -> Lemma: ,\n",
            "Token: privacy -> Lemma: privacy\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: any -> Lemma: any\n",
            "Token: intellectual -> Lemma: intellectual\n",
            "Token: property -> Lemma: property\n",
            "Token: rights -> Lemma: right\n",
            "Token: , -> Lemma: ,\n",
            "Token: as -> Lemma: as\n",
            "Token: are -> Lemma: be\n",
            "Token: or -> Lemma: or\n",
            "Token: reasonably -> Lemma: reasonably\n",
            "Token: may -> Lemma: may\n",
            "Token: be -> Lemma: be\n",
            "Token: expected -> Lemma: expect\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: necessary -> Lemma: necessary\n",
            "Token: for -> Lemma: for\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: to -> Lemma: to\n",
            "Token: exercise -> Lemma: exercise\n",
            "Token: its -> Lemma: its\n",
            "Token: rights -> Lemma: right\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: as -> Lemma: as\n",
            "Token: permitted -> Lemma: permit\n",
            "Token: under -> Lemma: under\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: without -> Lemma: without\n",
            "Token: incurring -> Lemma: incur\n",
            "Token: any -> Lemma: any\n",
            "Token: payment -> Lemma: payment\n",
            "Token: or -> Lemma: or\n",
            "Token: other -> Lemma: other\n",
            "Token: obligation -> Lemma: obligation\n",
            "Token: to -> Lemma: to\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: violating -> Lemma: violate\n",
            "Token: any -> Lemma: any\n",
            "Token: right -> Lemma: right\n",
            "Token: of -> Lemma: of\n",
            "Token: , -> Lemma: ,\n",
            "Token: any -> Lemma: any\n",
            "Token: such -> Lemma: such\n",
            "Token: person -> Lemma: person\n",
            "Token: or -> Lemma: or\n",
            "Token: entity -> Lemma: entity\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Copyright -> Lemma: Copyright\n",
            "Token: Notices -> Lemma: Notices\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: shall -> Lemma: shall\n",
            "Token: ensure -> Lemma: ensure\n",
            "Token: that -> Lemma: that\n",
            "Token: its -> Lemma: its\n",
            "Token: use -> Lemma: use\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: Work -> Lemma: work\n",
            "Token: is -> Lemma: be\n",
            "Token: marked -> Lemma: mark\n",
            "Token: with -> Lemma: with\n",
            "Token: the -> Lemma: the\n",
            "Token: appropriate -> Lemma: appropriate\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: notices -> Lemma: notice\n",
            "Token: specified -> Lemma: specify\n",
            "Token: by -> Lemma: by\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: in -> Lemma: in\n",
            "Token: a -> Lemma: a\n",
            "Token: reasonably -> Lemma: reasonably\n",
            "Token: prominent -> Lemma: prominent\n",
            "Token: position -> Lemma: position\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: order -> Lemma: order\n",
            "Token: and -> Lemma: and\n",
            "Token: manner -> Lemma: manner\n",
            "Token: provided -> Lemma: provide\n",
            "Token: by -> Lemma: by\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: shall -> Lemma: shall\n",
            "Token: abide -> Lemma: abide\n",
            "Token: by -> Lemma: by\n",
            "Token: the -> Lemma: the\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: laws -> Lemma: law\n",
            "Token: and -> Lemma: and\n",
            "Token: what -> Lemma: what\n",
            "Token: are -> Lemma: be\n",
            "Token: considered -> Lemma: consider\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: sound -> Lemma: sound\n",
            "Token: practices -> Lemma: practice\n",
            "Token: for -> Lemma: for\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: notice -> Lemma: notice\n",
            "Token: provisions -> Lemma: provision\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: Territory -> Lemma: Territory\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: shall -> Lemma: shall\n",
            "Token: not -> Lemma: not\n",
            "Token: use -> Lemma: use\n",
            "Token: any -> Lemma: any\n",
            "Token: copyright -> Lemma: copyright\n",
            "Token: notices -> Lemma: notice\n",
            "Token: that -> Lemma: that\n",
            "Token: conflict -> Lemma: conflict\n",
            "Token: with -> Lemma: with\n",
            "Token: , -> Lemma: ,\n",
            "Token: confuse -> Lemma: confuse\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: negate -> Lemma: negate\n",
            "Token: the -> Lemma: the\n",
            "Token: notices -> Lemma: notice\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: provides -> Lemma: provide\n",
            "Token: and -> Lemma: and\n",
            "Token: requires -> Lemma: require\n",
            "Token: hereunder -> Lemma: hereunder\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Payment -> Lemma: Payment\n",
            "Token: . -> Lemma: .\n",
            "Token: As -> Lemma: as\n",
            "Token: consideration -> Lemma: consideration\n",
            "Token: in -> Lemma: in\n",
            "Token: full -> Lemma: full\n",
            "Token: for -> Lemma: for\n",
            "Token: the -> Lemma: the\n",
            "Token: rights -> Lemma: right\n",
            "Token: granted -> Lemma: grant\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: shall -> Lemma: shall\n",
            "Token: pay -> Lemma: pay\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: a -> Lemma: a\n",
            "Token: one -> Lemma: one\n",
            "Token: - -> Lemma: -\n",
            "Token: time -> Lemma: time\n",
            "Token: fee -> Lemma: fee\n",
            "Token: in -> Lemma: in\n",
            "Token: the -> Lemma: the\n",
            "Token: amount -> Lemma: amount\n",
            "Token: of -> Lemma: of\n",
            "Token: \" -> Lemma: \"\n",
            "Token: one -> Lemma: one\n",
            "Token: hundred -> Lemma: hundred\n",
            "Token: US -> Lemma: US\n",
            "Token: Dollars -> Lemma: Dollars\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ( -> Lemma: (\n",
            "Token: 100.0 -> Lemma: 100.0\n",
            "Token: USD -> Lemma: USD\n",
            "Token: ) -> Lemma: )\n",
            "Token: upon -> Lemma: upon\n",
            "Token: execution -> Lemma: execution\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: payable -> Lemma: payable\n",
            "Token: as -> Lemma: as\n",
            "Token: follows -> Lemma: follow\n",
            "Token: : -> Lemma: :\n",
            "Token: \" -> Lemma: \"\n",
            "Token: bank -> Lemma: bank\n",
            "Token: transfer -> Lemma: transfer\n",
            "Token: \" -> Lemma: \"\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: General -> Lemma: General\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Interpretation -> Lemma: Interpretation\n",
            "Token: . -> Lemma: .\n",
            "Token: For -> Lemma: for\n",
            "Token: purposes -> Lemma: purpose\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: ( -> Lemma: (\n",
            "Token: a -> Lemma: a\n",
            "Token: ) -> Lemma: )\n",
            "Token: the -> Lemma: the\n",
            "Token: words -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: include -> Lemma: include\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: includes -> Lemma: include\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: and -> Lemma: and\n",
            "Token: \" -> Lemma: \"\n",
            "Token: including -> Lemma: include\n",
            "Token: \" -> Lemma: \"\n",
            "Token: are -> Lemma: be\n",
            "Token: deemed -> Lemma: deem\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: followed -> Lemma: follow\n",
            "Token: by -> Lemma: by\n",
            "Token: the -> Lemma: the\n",
            "Token: words -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: without -> Lemma: without\n",
            "Token: limitation -> Lemma: limitation\n",
            "Token: \" -> Lemma: \"\n",
            "Token: ; -> Lemma: ;\n",
            "Token: ( -> Lemma: (\n",
            "Token: b -> Lemma: b\n",
            "Token: ) -> Lemma: )\n",
            "Token: the -> Lemma: the\n",
            "Token: word -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: or -> Lemma: or\n",
            "Token: \" -> Lemma: \"\n",
            "Token: is -> Lemma: be\n",
            "Token: not -> Lemma: not\n",
            "Token: exclusive -> Lemma: exclusive\n",
            "Token: ; -> Lemma: ;\n",
            "Token: and -> Lemma: and\n",
            "Token: ( -> Lemma: (\n",
            "Token: c -> Lemma: c\n",
            "Token: ) -> Lemma: )\n",
            "Token: the -> Lemma: the\n",
            "Token: words -> Lemma: word\n",
            "Token: \" -> Lemma: \"\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereof -> Lemma: hereof\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereto -> Lemma: hereto\n",
            "Token: , -> Lemma: ,\n",
            "Token: \" -> Lemma: \"\n",
            "Token: and -> Lemma: and\n",
            "Token: \" -> Lemma: \"\n",
            "Token: hereunder -> Lemma: hereunder\n",
            "Token: \" -> Lemma: \"\n",
            "Token: refer -> Lemma: refer\n",
            "Token: to -> Lemma: to\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: as -> Lemma: as\n",
            "Token: a -> Lemma: a\n",
            "Token: whole -> Lemma: whole\n",
            "Token: . -> Lemma: .\n",
            "Token: This -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: is -> Lemma: be\n",
            "Token: intended -> Lemma: intend\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: construed -> Lemma: construe\n",
            "Token: without -> Lemma: without\n",
            "Token: regard -> Lemma: regard\n",
            "Token: to -> Lemma: to\n",
            "Token: any -> Lemma: any\n",
            "Token: presumption -> Lemma: presumption\n",
            "Token: or -> Lemma: or\n",
            "Token: rule -> Lemma: rule\n",
            "Token: requiring -> Lemma: require\n",
            "Token: construction -> Lemma: construction\n",
            "Token: or -> Lemma: or\n",
            "Token: interpretation -> Lemma: interpretation\n",
            "Token: against -> Lemma: against\n",
            "Token: the -> Lemma: the\n",
            "Token: party -> Lemma: party\n",
            "Token: drafting -> Lemma: draft\n",
            "Token: an -> Lemma: an\n",
            "Token: instrument -> Lemma: instrument\n",
            "Token: or -> Lemma: or\n",
            "Token: causing -> Lemma: cause\n",
            "Token: any -> Lemma: any\n",
            "Token: instrument -> Lemma: instrument\n",
            "Token: to -> Lemma: to\n",
            "Token: be -> Lemma: be\n",
            "Token: drafted -> Lemma: draft\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Entire -> Lemma: Entire\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: . -> Lemma: .\n",
            "Token: This -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: including -> Lemma: include\n",
            "Token: and -> Lemma: and\n",
            "Token: together -> Lemma: together\n",
            "Token: with -> Lemma: with\n",
            "Token: any -> Lemma: any\n",
            "Token: related -> Lemma: related\n",
            "Token: attachments -> Lemma: attachment\n",
            "Token: , -> Lemma: ,\n",
            "Token: constitutes -> Lemma: constitute\n",
            "Token: the -> Lemma: the\n",
            "Token: sole -> Lemma: sole\n",
            "Token: and -> Lemma: and\n",
            "Token: entire -> Lemma: entire\n",
            "Token: agreement -> Lemma: agreement\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: with -> Lemma: with\n",
            "Token: respect -> Lemma: respect\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: subject -> Lemma: subject\n",
            "Token: matter -> Lemma: matter\n",
            "Token: contained -> Lemma: contain\n",
            "Token: herein -> Lemma: herein\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: supersedes -> Lemma: supersede\n",
            "Token: all -> Lemma: all\n",
            "Token: prior -> Lemma: prior\n",
            "Token: and -> Lemma: and\n",
            "Token: contemporaneous -> Lemma: contemporaneous\n",
            "Token: understandings -> Lemma: understanding\n",
            "Token: , -> Lemma: ,\n",
            "Token: agreements -> Lemma: agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: representations -> Lemma: representation\n",
            "Token: , -> Lemma: ,\n",
            "Token: and -> Lemma: and\n",
            "Token: warranties -> Lemma: warranty\n",
            "Token: , -> Lemma: ,\n",
            "Token: both -> Lemma: both\n",
            "Token: written -> Lemma: write\n",
            "Token: and -> Lemma: and\n",
            "Token: oral -> Lemma: oral\n",
            "Token: , -> Lemma: ,\n",
            "Token: with -> Lemma: with\n",
            "Token: respect -> Lemma: respect\n",
            "Token: to -> Lemma: to\n",
            "Token: such -> Lemma: such\n",
            "Token: subject -> Lemma: subject\n",
            "Token: matter -> Lemma: matter\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Severability -> Lemma: Severability\n",
            "Token: . -> Lemma: .\n",
            "Token: If -> Lemma: if\n",
            "Token: any -> Lemma: any\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: is -> Lemma: be\n",
            "Token: invalid -> Lemma: invalid\n",
            "Token: , -> Lemma: ,\n",
            "Token: illegal -> Lemma: illegal\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: unenforceable -> Lemma: unenforceable\n",
            "Token: in -> Lemma: in\n",
            "Token: any -> Lemma: any\n",
            "Token: jurisdiction -> Lemma: jurisdiction\n",
            "Token: , -> Lemma: ,\n",
            "Token: such -> Lemma: such\n",
            "Token: invalidity -> Lemma: invalidity\n",
            "Token: , -> Lemma: ,\n",
            "Token: illegality -> Lemma: illegality\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: unenforceability -> Lemma: unenforceability\n",
            "Token: will -> Lemma: will\n",
            "Token: not -> Lemma: not\n",
            "Token: affect -> Lemma: affect\n",
            "Token: the -> Lemma: the\n",
            "Token: enforceability -> Lemma: enforceability\n",
            "Token: of -> Lemma: of\n",
            "Token: any -> Lemma: any\n",
            "Token: other -> Lemma: other\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: of -> Lemma: of\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: invalidate -> Lemma: invalidate\n",
            "Token: or -> Lemma: or\n",
            "Token: render -> Lemma: render\n",
            "Token: unenforceable -> Lemma: unenforceable\n",
            "Token: such -> Lemma: such\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: in -> Lemma: in\n",
            "Token: any -> Lemma: any\n",
            "Token: other -> Lemma: other\n",
            "Token: jurisdiction -> Lemma: jurisdiction\n",
            "Token: . -> Lemma: .\n",
            "Token: [ -> Lemma: [\n",
            "Token: Upon -> Lemma: upon\n",
            "Token: a -> Lemma: a\n",
            "Token: determination -> Lemma: determination\n",
            "Token: that -> Lemma: that\n",
            "Token: any -> Lemma: any\n",
            "Token: term -> Lemma: term\n",
            "Token: or -> Lemma: or\n",
            "Token: provision -> Lemma: provision\n",
            "Token: is -> Lemma: be\n",
            "Token: invalid -> Lemma: invalid\n",
            "Token: , -> Lemma: ,\n",
            "Token: illegal -> Lemma: illegal\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: unenforceable -> Lemma: unenforceable\n",
            "Token: , -> Lemma: ,\n",
            "Token: [ -> Lemma: [\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: shall -> Lemma: shall\n",
            "Token: negotiate -> Lemma: negotiate\n",
            "Token: in -> Lemma: in\n",
            "Token: good -> Lemma: good\n",
            "Token: faith -> Lemma: faith\n",
            "Token: to -> Lemma: to\n",
            "Token: / -> Lemma: /\n",
            "Token: the -> Lemma: the\n",
            "Token: court -> Lemma: court\n",
            "Token: may -> Lemma: may\n",
            "Token: ] -> Lemma: ]\n",
            "Token: modify -> Lemma: modify\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: to -> Lemma: to\n",
            "Token: effect -> Lemma: effect\n",
            "Token: the -> Lemma: the\n",
            "Token: original -> Lemma: original\n",
            "Token: intent -> Lemma: intent\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: as -> Lemma: as\n",
            "Token: closely -> Lemma: closely\n",
            "Token: as -> Lemma: as\n",
            "Token: possible -> Lemma: possible\n",
            "Token: in -> Lemma: in\n",
            "Token: order -> Lemma: order\n",
            "Token: that -> Lemma: that\n",
            "Token: the -> Lemma: the\n",
            "Token: transactions -> Lemma: transaction\n",
            "Token: contemplated -> Lemma: contemplate\n",
            "Token: hereby -> Lemma: hereby\n",
            "Token: be -> Lemma: be\n",
            "Token: consummated -> Lemma: consummate\n",
            "Token: as -> Lemma: as\n",
            "Token: originally -> Lemma: originally\n",
            "Token: contemplated -> Lemma: contemplate\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: greatest -> Lemma: great\n",
            "Token: extent -> Lemma: extent\n",
            "Token: possible -> Lemma: possible\n",
            "Token: . -> Lemma: .\n",
            "Token: ] -> Lemma: ]\n",
            "Token: \n",
            "\n",
            " -> Lemma: \n",
            "\n",
            "\n",
            "Token: Assignment -> Lemma: Assignment\n",
            "Token: . -> Lemma: .\n",
            "Token: Licensee -> Lemma: license\n",
            "Token: may -> Lemma: may\n",
            "Token: freely -> Lemma: freely\n",
            "Token: assign -> Lemma: assign\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: transfer -> Lemma: transfer\n",
            "Token: all -> Lemma: all\n",
            "Token: or -> Lemma: or\n",
            "Token: any -> Lemma: any\n",
            "Token: of -> Lemma: of\n",
            "Token: its -> Lemma: its\n",
            "Token: rights -> Lemma: right\n",
            "Token: , -> Lemma: ,\n",
            "Token: or -> Lemma: or\n",
            "Token: delegate -> Lemma: delegate\n",
            "Token: or -> Lemma: or\n",
            "Token: otherwise -> Lemma: otherwise\n",
            "Token: transfer -> Lemma: transfer\n",
            "Token: all -> Lemma: all\n",
            "Token: or -> Lemma: or\n",
            "Token: any -> Lemma: any\n",
            "Token: of -> Lemma: of\n",
            "Token: its -> Lemma: its\n",
            "Token: obligations -> Lemma: obligation\n",
            "Token: or -> Lemma: or\n",
            "Token: performance -> Lemma: performance\n",
            "Token: , -> Lemma: ,\n",
            "Token: under -> Lemma: under\n",
            "Token: this -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: without -> Lemma: without\n",
            "Token: Licensor -> Lemma: license\n",
            "Token: 's -> Lemma: 's\n",
            "Token: consent -> Lemma: consent\n",
            "Token: . -> Lemma: .\n",
            "Token: This -> Lemma: this\n",
            "Token: Agreement -> Lemma: Agreement\n",
            "Token: is -> Lemma: be\n",
            "Token: binding -> Lemma: bind\n",
            "Token: upon -> Lemma: upon\n",
            "Token: and -> Lemma: and\n",
            "Token: inures -> Lemma: inure\n",
            "Token: to -> Lemma: to\n",
            "Token: the -> Lemma: the\n",
            "Token: benefit -> Lemma: benefit\n",
            "Token: of -> Lemma: of\n",
            "Token: the -> Lemma: the\n",
            "Token: parties -> Lemma: party\n",
            "Token: hereto -> Lemma: hereto\n",
            "Token: and -> Lemma: and\n",
            "Token: their -> Lemma: their\n",
            "Token: respective -> Lemma: respective\n",
            "Token: permitted -> Lemma: permit\n",
            "Token: successors -> Lemma: successor\n",
            "Token: and -> Lemma: and\n",
            "Token: assigns -> Lemma: assign\n",
            "Token: . -> Lemma: .\n",
            "Token: \n",
            " -> Lemma: \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}